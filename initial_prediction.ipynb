{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Computation device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from datetime import date, datetime, timedelta\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from typing import Annotated\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import pytz\n",
    "import talib\n",
    "import torch\n",
    "from torch import nn, autograd\n",
    "import torchmetrics\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torchviz import make_dot\n",
    "\n",
    "import get_data\n",
    "from tools import dataframe_reformat, inspect_code, plotting, training, wandb_api\n",
    "\n",
    "log_wandb = True\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {\"num_workers\": 2, \"pin_memory\": True} if use_cuda else {\"num_workers\": 4}\n",
    "print(f\"[INFO]: Computation device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatiasetcheverry\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/matias/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/matias/binance/wandb/run-20220313_174611-8yn1a0mt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/matiasetcheverry/binance/runs/8yn1a0mt\" target=\"_blank\">sandy-water-19</a></strong> to <a href=\"https://wandb.ai/matiasetcheverry/binance\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if log_wandb:\n",
    "    import wandb\n",
    "\n",
    "    wandb_api.login()\n",
    "    run = wandb.init(\n",
    "        project=\"binance\",\n",
    "        group=\"Initial GAN\",\n",
    "        job_type=\"test\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if log_wandb:\n",
    "    config = wandb.config\n",
    "else:\n",
    "    config = {}\n",
    "\n",
    "\n",
    "config[\"job_type\"] = run.job_type if \"run\" in locals() else \"test\"\n",
    "config[\"log_wandb\"] = log_wandb\n",
    "config[\"device\"] = device\n",
    "config[\"train_test_split\"] = 0.7\n",
    "config[\"nb_previous_close\"] = 20\n",
    "config[\"batch_size\"] = 16\n",
    "config[\"learning_rate_generator\"] = 0.0001\n",
    "config[\"learning_rate_discriminator\"] = 0.0004\n",
    "config[\"beta1\"] = 0.5\n",
    "config[\"beta2\"] = 0.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        csv_file=None,\n",
    "        train_df=None,\n",
    "        test_df=None,\n",
    "        train_dataset=None,\n",
    "        validation_dataset=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        if csv_file is not None:\n",
    "            self.df = pd.read_csv(csv_file, delimiter=\";\")\n",
    "            self.df[\"BEGINNING_DATE\"] = pd.to_datetime(\n",
    "                self.df[\"BEGINNING_DATE\"], dayfirst=True\n",
    "            )\n",
    "            self.df[\"ENDING_DATE\"] = pd.to_datetime(\n",
    "                self.df[\"ENDING_DATE\"], dayfirst=True\n",
    "            )\n",
    "            self.df[\"TICKER\"] += \"-USD\"\n",
    "\n",
    "        self.train_df = train_df.convert_dtypes() if train_df is not None else None\n",
    "        self.test_df = test_df.convert_dtypes() if test_df is not None else None\n",
    "        self.train_dataset = train_dataset\n",
    "        self.validation_dataset = validation_dataset\n",
    "\n",
    "    def preprocess_klines(\n",
    "        self,\n",
    "        data=None,\n",
    "        ticker=None,\n",
    "        beginning_date=None,\n",
    "        ending_date=None,\n",
    "        interval=\"1d\",\n",
    "    ):\n",
    "        if data is None:\n",
    "            data = get_data.select_data(\n",
    "                ticker,\n",
    "                interval,\n",
    "                beginning_date=beginning_date,\n",
    "                ending_date=ending_date,\n",
    "            )\n",
    "        data.dropna(axis=0, inplace=True)\n",
    "        data.drop(labels=\"Date\", axis=1, inplace=True)\n",
    "        data.replace(\n",
    "            to_replace=[np.inf, -np.inf, np.float64(\"inf\"), -np.float64(\"inf\")],\n",
    "            value=0,\n",
    "            inplace=True,\n",
    "        )\n",
    "        idx_close = list(data.columns).index(\"Close\")\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        klines = torch.FloatTensor(scaler.fit_transform(data))\n",
    "\n",
    "        data_close = klines[:, idx_close]\n",
    "        single_close = torch.FloatTensor(data_close[self.config[\"nb_previous_close\"] :])\n",
    "        multiple_close = torch.stack(\n",
    "            [\n",
    "                torch.FloatTensor(data_close[i : i + self.config[\"nb_previous_close\"]])\n",
    "                for i in range(len(data_close) - self.config[\"nb_previous_close\"])\n",
    "            ]\n",
    "        )\n",
    "        multiple_klines = torch.stack(\n",
    "            [\n",
    "                klines[i : i + self.config[\"nb_previous_close\"], :]\n",
    "                for i in range(len(klines) - self.config[\"nb_previous_close\"])\n",
    "            ]\n",
    "        )\n",
    "        return multiple_klines, single_close, multiple_close\n",
    "\n",
    "    def prepare_data(self):\n",
    "        for _, row in self.df.iterrows():\n",
    "            _ = get_data.select_data(\n",
    "                row[\"TICKER\"],\n",
    "                \"1d\",\n",
    "                beginning_date=row[\"BEGINNING_DATE\"],\n",
    "                ending_date=row[\"ENDING_DATE\"],\n",
    "            )\n",
    "\n",
    "    def setup(self, stage):\n",
    "        klines_training_sets = []\n",
    "        single_close_training_sets = []\n",
    "        multiple_close_training_sets = []\n",
    "        klines_validation_sets = []\n",
    "        single_close_validation_sets = []\n",
    "        multiple_close_validation_sets = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            klines, single_close, multiple_close = self.preprocess_klines(\n",
    "                ticker=row[\"TICKER\"],\n",
    "                beginning_date=row[\"BEGINNING_DATE\"],\n",
    "                ending_date=row[\"ENDING_DATE\"],\n",
    "            )\n",
    "            n = len(klines)\n",
    "            klines_training_sets.append(\n",
    "                klines[: int(n * self.config[\"train_test_split\"]), :, :]\n",
    "            )\n",
    "            single_close_training_sets.append(\n",
    "                single_close[: int(n * self.config[\"train_test_split\"])]\n",
    "            )\n",
    "            multiple_close_training_sets.append(\n",
    "                multiple_close[: int(n * self.config[\"train_test_split\"])]\n",
    "            )\n",
    "\n",
    "            klines_validation_sets.append(\n",
    "                klines[int(n * self.config[\"train_test_split\"]) :, :, :]\n",
    "            )\n",
    "            single_close_validation_sets.append(\n",
    "                single_close[int(n * self.config[\"train_test_split\"]) :]\n",
    "            )\n",
    "            multiple_close_validation_sets.append(\n",
    "                multiple_close[int(n * self.config[\"train_test_split\"]) :]\n",
    "            )\n",
    "        assert len(klines_training_sets) == len(single_close_validation_sets)\n",
    "        assert len(klines_training_sets) == len(multiple_close_training_sets)\n",
    "        self.klines_training_sets = torch.cat(klines_training_sets)\n",
    "        self.single_close_training_sets = torch.cat(\n",
    "            single_close_training_sets\n",
    "        ).unsqueeze(-1)\n",
    "        self.multiple_close_training_sets = torch.cat(multiple_close_training_sets)\n",
    "\n",
    "        self.klines_validation_sets = torch.cat(klines_validation_sets)\n",
    "        self.single_close_validation_sets = torch.cat(\n",
    "            single_close_validation_sets\n",
    "        ).unsqueeze(-1)\n",
    "        self.multiple_close_validation_sets = torch.cat(multiple_close_validation_sets)\n",
    "\n",
    "        self.train_dataset = TensorDataset(\n",
    "            self.klines_training_sets,\n",
    "            self.multiple_close_training_sets,\n",
    "            self.single_close_training_sets,\n",
    "        )\n",
    "        self.validation_dataset = TensorDataset(\n",
    "            self.klines_validation_sets,\n",
    "            self.multiple_close_validation_sets,\n",
    "            self.single_close_validation_sets,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.config[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.validation_dataset,\n",
    "            batch_size=self.config[\"batch_size\"],\n",
    "            shuffle=False,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.val_dataloader()\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(\n",
    "            [image for image, _ in self.validation_dataset],\n",
    "            batch_size=self.config[\"batch_size\"],\n",
    "            shuffle=False,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "dm = DataModule(config, \"DATE.csv\")\n",
    "dm.prepare_data()\n",
    "dm.setup(stage=\"fit\")\n",
    "train_dataloader = dm.train_dataloader()\n",
    "val_dataloader = dm.val_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8032\n",
      "3472\n",
      "torch.Size([16, 20, 29]) torch.Size([16, 20]) torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader) * config[\"batch_size\"])\n",
    "print(len(val_dataloader) * config[\"batch_size\"])\n",
    "klines, single_close, multiple_close = next(iter(train_dataloader))\n",
    "print(klines.shape, single_close.shape, multiple_close.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matias/.pyenv/versions/3.9.8/envs/binance/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/matias/.pyenv/versions/3.9.8/envs/binance/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:469: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "\n",
      "  | Name          | Type          | Params\n",
      "------------------------------------------------\n",
      "0 | generator     | Generator     | 14.4 K\n",
      "1 | discriminator | Discriminator | 84.3 K\n",
      "------------------------------------------------\n",
      "98.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "98.7 K    Total params\n",
      "0.395     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: not saving models.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf0e6717a8b4686a57565a4136aae99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b7750c08764c089805d5d0eed909be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea546bc342b949b6a624b019dff99a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5386ea9fb8a74bc5a9355a72a6359a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d72eb491eb54be582657bfbe4f65ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv1d(config[\"nb_previous_close\"], 32, kernel_size=2),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.lstm_layer = nn.LSTM(\n",
    "            28, 1, num_layers=64, batch_first=True, bidirectional=True, dropout=0.3\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            # nn.Flatten(),\n",
    "            nn.Linear(in_features=2 * 64, out_features=64),\n",
    "            nn.LeakyReLU(),\n",
    "            # nn.Dropout2d(0.2),\n",
    "            nn.Linear(in_features=64, out_features=32),\n",
    "            nn.LeakyReLU(),\n",
    "            # nn.Dropout2d(0.2),\n",
    "            nn.Linear(in_features=32, out_features=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        output, (hidden_state, cell_state) = self.lstm_layer(x)\n",
    "        hidden_state = torch.permute(hidden_state, (1, 0, 2)).reshape(-1, 2 * 64)\n",
    "        x = self.fc_layers(hidden_state)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=2),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        _, nb_filters, width = self.conv_layers(torch.rand(1, 1, 21)).shape\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=nb_filters * width, out_features=64),\n",
    "            nn.LeakyReLU(),\n",
    "            # nn.Dropout2d(0.2),\n",
    "            nn.Linear(in_features=64, out_features=32),\n",
    "            nn.LeakyReLU(),\n",
    "            # nn.Dropout2d(0.2),\n",
    "            nn.Linear(in_features=32, out_features=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GAN(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # networks\n",
    "        self.generator = Generator(self.config)\n",
    "        self.discriminator = Discriminator(self.config)\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def adversarial_loss(self, y_hat, y):\n",
    "        return nn.BCELoss()(y_hat, y)\n",
    "\n",
    "    def generator_loss(self, fake, y_hat, y):\n",
    "        fake_generator = torch.mean(fake)\n",
    "        close_generator = torchmetrics.MeanSquaredError()(y_hat, y)\n",
    "        sign_generator = torch.mean(torch.abs(torch.sign(y_hat) - torch.sign(y)))\n",
    "        return fake_generator, close_generator, sign_generator\n",
    "\n",
    "    def discriminator_loss(self, real, fake, y_hat, y):\n",
    "        fake_discriminator = torch.mean(fake)\n",
    "        real_discriminator = torch.mean(real)\n",
    "        return fake_discriminator, real_discriminator\n",
    "\n",
    "    def _training_step_generator(self, batch, opt=None):\n",
    "        klines, multiple_close, y = batch\n",
    "        fake_close = self(klines)\n",
    "        fake = self.discriminator(torch.cat([multiple_close, fake_close], dim=1))\n",
    "        fake_generator, close_generator, sign_generator = self.generator_loss(\n",
    "            fake, fake_close, y\n",
    "        )\n",
    "        g_loss = -fake_generator + 0.5 * close_generator + 0.5 * sign_generator\n",
    "        if opt is not None:\n",
    "            opt.zero_grad()\n",
    "            self.manual_backward(g_loss)\n",
    "            opt.step()\n",
    "\n",
    "        return {\n",
    "            \"g_loss\": g_loss,\n",
    "            \"fake_generator\": fake_generator,\n",
    "            \"close_generator\": close_generator,\n",
    "            \"sign_generator\": sign_generator,\n",
    "        }\n",
    "\n",
    "    def compute_gradient_penalty(self, real, fake):\n",
    "        torch.autograd.set_grad_enabled(True)\n",
    "        alpha = torch.rand(real.shape[0])\n",
    "        interpolates = (\n",
    "            (alpha * real.transpose(0, 1) + ((1 - alpha) * fake.transpose(0, 1)))\n",
    "            .transpose(0, 1)\n",
    "            .to(self.config[\"device\"])\n",
    "        )\n",
    "\n",
    "        interpolates = autograd.Variable(interpolates, requires_grad=True)\n",
    "        disc_interpolates = self.discriminator(interpolates)\n",
    "        gradients = autograd.grad(\n",
    "            outputs=disc_interpolates,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=torch.ones(disc_interpolates.size()).to(self.config[\"device\"]),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True,\n",
    "        )[0]\n",
    "        gradients = gradients.view(gradients.size(0), -1)\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        return gradient_penalty\n",
    "\n",
    "    def _training_step_discriminator(self, batch, opt=None, steps=5):\n",
    "        klines, multiple_close, y = batch\n",
    "        D_loss = 0\n",
    "        Fake_discriminator = 0\n",
    "        Real_discriminator = 0\n",
    "        for _ in range(steps):\n",
    "            fake_close = self(klines)\n",
    "            fake = self.discriminator(torch.cat([multiple_close, fake_close], dim=1))\n",
    "            real = self.discriminator(torch.cat([multiple_close, y], dim=1))\n",
    "            fake_discriminator, real_discriminator = self.discriminator_loss(\n",
    "                real, fake, fake_close, y\n",
    "            )\n",
    "            gradient_penalty = self.compute_gradient_penalty(\n",
    "                torch.cat([multiple_close, y], dim=1),\n",
    "                torch.cat([multiple_close, fake_close], dim=1),\n",
    "            )\n",
    "            d_loss = fake_discriminator - real_discriminator + 10 * gradient_penalty\n",
    "            if opt is not None:\n",
    "                opt.zero_grad()\n",
    "                self.manual_backward(d_loss)\n",
    "                opt.step()\n",
    "\n",
    "            D_loss += d_loss / steps\n",
    "            Fake_discriminator += fake_discriminator / steps\n",
    "            Real_discriminator += real_discriminator / steps\n",
    "\n",
    "        return {\n",
    "            \"d_loss\": D_loss,\n",
    "            \"penalty_discriminator\": gradient_penalty,\n",
    "            \"fake_discriminator\": Fake_discriminator,\n",
    "            \"real_discriminator\": Real_discriminator,\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        opt_g, opt_d = self.optimizers()\n",
    "\n",
    "        metrics = {}\n",
    "        metrics.update(self._training_step_generator(batch, opt_g))\n",
    "        metrics.update(self._training_step_discriminator(batch, opt_d, steps=5))\n",
    "\n",
    "        self.log_dict(\n",
    "            metrics,\n",
    "            prog_bar=True,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        metrics = {}\n",
    "        metrics.update(self._training_step_generator(batch))\n",
    "        metrics.update(self._training_step_discriminator(batch, steps=1))\n",
    "        metrics = {\n",
    "            \"val_\" + metric_name: metric_value\n",
    "            for metric_name, metric_value in metrics.items()\n",
    "        }\n",
    "\n",
    "        self.log_dict(\n",
    "            metrics,\n",
    "            prog_bar=True,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "        return metrics\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt_g = torch.optim.Adam(\n",
    "            self.generator.parameters(),\n",
    "            lr=self.config[\"learning_rate_generator\"],\n",
    "            betas=(self.config[\"beta1\"], self.config[\"beta2\"]),\n",
    "        )\n",
    "        opt_d = torch.optim.Adam(\n",
    "            self.discriminator.parameters(),\n",
    "            lr=self.config[\"learning_rate_discriminator\"],\n",
    "            betas=(self.config[\"beta1\"], self.config[\"beta2\"]),\n",
    "        )\n",
    "        return opt_g, opt_d\n",
    "\n",
    "\n",
    "model = GAN(config)\n",
    "\n",
    "model_checkpoint = pl.callbacks.model_checkpoint.ModelCheckpoint(\n",
    "    dirpath=run.dir if \"run\" in locals() else \"tmp/\",\n",
    "    filename=\"{epoch}-{val_loss:.3f}\",\n",
    "    monitor=\"_generatorg_loss\",\n",
    "    mode=\"min\",\n",
    "    verbose=True,\n",
    "    save_last=True,\n",
    ")\n",
    "\n",
    "script_checkpoint = training.ScriptCheckpoint(\n",
    "    dirpath=run.dir if \"run\" in locals() else \"tmp/\",\n",
    ")\n",
    "\n",
    "callbacks = [script_checkpoint]\n",
    "log = None\n",
    "if config[\"job_type\"] == \"train\" or False:\n",
    "    callbacks.append(model_checkpoint)\n",
    "    print(f\"[INFO]: saving models.\")\n",
    "else:\n",
    "    print(f\"[INFO]: not saving models.\")\n",
    "if config[\"job_type\"] == \"debug\":\n",
    "    log = \"all\"\n",
    "\n",
    "if config[\"log_wandb\"]:\n",
    "    wandb_logger = pl.loggers.WandbLogger()\n",
    "    wandb_logger.watch(model, log=log, log_graph=True)\n",
    "else:\n",
    "    wandb_logger = None\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=250,\n",
    "    callbacks=callbacks,\n",
    "    logger=wandb_logger,\n",
    "    devices=\"auto\",\n",
    "    accelerator=\"auto\",\n",
    "    limit_train_batches=0.3,\n",
    "    limit_val_batches=0.3,\n",
    ")\n",
    "trainer.fit(model, dm)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "321abdbcb709f963ff456ab1955c8b6ac962fdf45411881beed91e0e00a7d370"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit ('binance')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
