{"cells":[{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2022-03-16T20:07:53.233736Z","iopub.status.busy":"2022-03-16T20:07:53.233393Z","iopub.status.idle":"2022-03-16T20:08:19.361866Z","shell.execute_reply":"2022-03-16T20:08:19.361083Z","shell.execute_reply.started":"2022-03-16T20:07:53.233653Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: yfinance in /home/matias/.pyenv/versions/3.9.8/envs/binance/lib/python3.9/site-packages (0.1.70)\n","Requirement already satisfied: numpy>=1.15 in /home/matias/.pyenv/versions/3.9.8/envs/binance/lib/python3.9/site-packages (from yfinance) (1.20.3)\n","Requirement already satisfied: requests>=2.26 in /home/matias/.pyenv/versions/3.9.8/envs/binance/lib/python3.9/site-packages (from yfinance) (2.27.0)\n","Requirement already satisfied: multitasking>=0.0.7 in /home/matias/.pyenv/versions/3.9.8/envs/binance/lib/python3.9/site-packages (from yfinance) (0.0.10)\n","Requirement already satisfied: pandas>=0.24.0 in /home/matias/.pyenv/versions/3.9.8/envs/binance/lib/python3.9/site-packages (from yfinance) (1.3.5)\n","Requirement already satisfied: lxml>=4.5.1 in /home/matias/.pyenv/versions/3.9.8/envs/binance/lib/python3.9/site-packages (from yfinance) (4.8.0)\n","Requirement already satisfied: pytz>=2017.3 in /home/matias/.pyenv/versions/3.9.8/envs/binance/lib/python3.9/site-packages (from pandas>=0.24.0->yfinance) (2021.3)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /home/matias/.pyenv/versions/3.9.8/envs/binance/lib/python3.9/site-packages (from pandas>=0.24.0->yfinance) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /home/matias/.pyenv/versions/3.9.8/envs/binance/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yfinance) (1.16.0)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /home/matias/.pyenv/versions/3.9.8/envs/binance/lib/python3.9/site-packages (from requests>=2.26->yfinance) (2.0.9)\n","Requirement already satisfied: idna<4,>=2.5 in /home/matias/.pyenv/versions/3.9.8/envs/binance/lib/python3.9/site-packages (from requests>=2.26->yfinance) (3.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/matias/.pyenv/versions/3.9.8/envs/binance/lib/python3.9/site-packages (from requests>=2.26->yfinance) (1.26.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/matias/.pyenv/versions/3.9.8/envs/binance/lib/python3.9/site-packages (from requests>=2.26->yfinance) (2021.10.8)\n","\u001b[33mWARNING: You are using pip version 21.2.4; however, version 22.0.4 is available.\n","You should consider upgrading via the '/home/matias/.pyenv/versions/3.9.8/envs/binance/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n","\u001b[31mERROR: Could not find a version that satisfies the requirement talib-binary (from versions: none)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for talib-binary\u001b[0m\n","\u001b[33mWARNING: You are using pip version 21.2.4; however, version 22.0.4 is available.\n","You should consider upgrading via the '/home/matias/.pyenv/versions/3.9.8/envs/binance/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"]}],"source":["!pip install yfinance\n","!pip install talib-binary"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class DataModule:\n","    def __init__(\n","        self,\n","        config,\n","        csv_file=None,\n","        train_df=None,\n","        test_df=None,\n","        train_dataset=None,\n","        validation_dataset=None,\n","    ):\n","        super().__init__()\n","        self.config = config\n","\n","        if csv_file is not None:\n","            self.df = pd.read_csv(csv_file, delimiter=\";\")\n","            self.df[\"BEGINNING_DATE\"] = pd.to_datetime(\n","                self.df[\"BEGINNING_DATE\"], dayfirst=True\n","            )\n","            self.df[\"ENDING_DATE\"] = pd.to_datetime(\n","                self.df[\"ENDING_DATE\"], dayfirst=True\n","            )\n","            self.df[\"TICKER\"] += \"-USD\"\n","\n","        self.train_df = train_df.convert_dtypes() if train_df is not None else None\n","        self.test_df = test_df.convert_dtypes() if test_df is not None else None\n","        self.train_dataset = train_dataset\n","        self.validation_dataset = validation_dataset\n","\n","    def _preprocess_klines(\n","        self,\n","        data=None,\n","        ticker=None,\n","        beginning_date=None,\n","        ending_date=None,\n","        interval=\"1d\",\n","    ):\n","        if data is None:\n","            data = get_data.select_data(\n","                ticker,\n","                interval,\n","                beginning_date=beginning_date,\n","                ending_date=ending_date,\n","            )\n","\n","        data = data.dropna(axis=0)\n","        data = data.drop(labels=\"Date\", axis=1)\n","        data = data.replace(\n","            to_replace=[np.inf, -np.inf, np.float64(\"inf\"), -np.float64(\"inf\")],\n","            value=0,\n","        )\n","        idx_close = list(data.columns).index(\"Close\")\n","        scaler = MinMaxScaler(feature_range=(-1, 1))\n","        klines = tf.convert_to_tensor(scaler.fit_transform(data), dtype=tf.float32)\n","        data_close = klines[:, idx_close]\n","        single_close = tf.convert_to_tensor(\n","            data_close[self.config[\"nb_previous_close\"] :]\n","        )\n","        multiple_close = tf.stack(\n","            [\n","                data_close[i : i + self.config[\"nb_previous_close\"]]\n","                for i in range(len(data_close) - self.config[\"nb_previous_close\"])\n","            ]\n","        )\n","        multiple_klines = tf.stack(\n","            [\n","                klines[i : i + self.config[\"nb_previous_close\"], :]\n","                for i in range(len(klines) - self.config[\"nb_previous_close\"])\n","            ]\n","        )\n","        return scaler, multiple_klines, single_close, multiple_close\n","\n","    def setup(self):\n","        klines = []\n","        single_closes = []\n","        multiple_closes = []\n","\n","        scalers = []\n","        test_klines = []\n","        test_single_closes = []\n","        test_multiple_closes = []\n","        for index, row in self.df.iloc[0:1, :].iterrows():\n","            scaler, kline, single_close, multiple_close = self._preprocess_klines(\n","                ticker=row[\"TICKER\"],\n","                beginning_date=row[\"BEGINNING_DATE\"],\n","                ending_date=row[\"ENDING_DATE\"],\n","            )\n","            if len(kline) > 0:\n","                n = int(len(kline) * (1 - self.config[\"train_val_test_split\"][-1]))\n","                klines.append(kline[:n])\n","                single_closes.append(single_close[:n])\n","                multiple_closes.append(multiple_close[:n])\n","\n","                test_klines.append(tf.data.Dataset.from_tensor_slices(kline[n:]))\n","                test_single_closes.append(\n","                    tf.data.Dataset.from_tensor_slices(\n","                        tf.expand_dims(\n","                            (tf.expand_dims(single_close[n:], axis=-1)), axis=-1\n","                        ),\n","                    )\n","                )\n","                test_multiple_closes.append(\n","                    tf.data.Dataset.from_tensor_slices(\n","                        tf.expand_dims(multiple_close[n:], axis=-1),\n","                    )\n","                )\n","                scalers.append(scaler)\n","\n","        klines = tf.data.Dataset.from_tensor_slices(\n","            tf.concat(klines, axis=0),\n","        )\n","        single_closes = tf.data.Dataset.from_tensor_slices(\n","            tf.expand_dims(\n","                (tf.expand_dims(tf.concat(single_closes, axis=0), axis=-1)), axis=-1\n","            ),\n","        )\n","        multiple_closes = tf.data.Dataset.from_tensor_slices(\n","            tf.expand_dims(tf.concat(multiple_closes, axis=0), axis=-1),\n","        )\n","        dataset = (\n","            tf.data.Dataset.zip((klines, multiple_closes, single_closes))\n","            .shuffle(len(klines), reshuffle_each_iteration=True)\n","            .batch(\n","                self.config[\"batch_size\"],\n","                drop_remainder=False,\n","                num_parallel_calls=tf.data.AUTOTUNE,\n","            )\n","        )\n","        self.test_datasets = [\n","            (\n","                scaler,\n","                tf.data.Dataset.zip((kline, multiple_close, single_close)).batch(\n","                    self.config[\"batch_size\"],\n","                    drop_remainder=False,\n","                    num_parallel_calls=tf.data.AUTOTUNE,\n","                ),\n","            )\n","            for scaler, kline, multiple_close, single_close in zip(\n","                scalers, test_klines, test_multiple_closes, test_single_closes\n","            )\n","        ]\n","\n","        train_size = int(\n","            config[\"train_test_split\"][0]\n","            / (1 - self.config[\"train_val_test_split\"][-1])\n","            * len(dataset)\n","        )\n","        self.train_dataset = dataset.take(train_size)\n","        self.val_dataset = dataset.skip(train_size)\n","\n","\n","dm = DataModule(config, repo_path / \"DATE.csv\")\n","dm.setup()\n","train_dataset = dm.train_dataset\n","test_datasets = dm.test_datasets\n","print(len(test_datasets))\n"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2022-03-16T20:08:19.364642Z","iopub.status.busy":"2022-03-16T20:08:19.364286Z","iopub.status.idle":"2022-03-16T20:08:29.291047Z","shell.execute_reply":"2022-03-16T20:08:29.290120Z","shell.execute_reply.started":"2022-03-16T20:08:19.364600Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'CPU'"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["import sys\n","from pathlib import Path\n","\n","IS_COLAB = \"google.colab\" in sys.modules\n","IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n","if IS_KAGGLE:\n","    repo_path = Path(\"../input/crypto-prediction\")\n","elif IS_COLAB:\n","    from google.colab import drive\n","\n","    drive.mount(\"/content/gdrive\")\n","    repo_path = Path(\"/content/gdrive/MyDrive/crypto-prediction\")\n","else:\n","    repo_path = Path(\"/home/matias/crypto-prediction\")\n","sys.path.append(str(repo_path))\n","\n","import numpy as np\n","import pandas as pd\n","import pytorch_lightning as pl\n","import tensorflow as tf\n","import yfinance as yf\n","from sklearn.preprocessing import MinMaxScaler\n","from torch.utils.data import DataLoader, Dataset, TensorDataset\n","\n","import get_data\n","import wandb\n","from tools import dataframe_reformat, inspect_code, plotting, training, wandb_api\n","\n","log_wandb = True\n","tf.config.list_physical_devices(\"CPU\")[0].device_type\n"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2022-03-16T20:08:29.292443Z","iopub.status.busy":"2022-03-16T20:08:29.292229Z","iopub.status.idle":"2022-03-16T20:08:39.625651Z","shell.execute_reply":"2022-03-16T20:08:39.624722Z","shell.execute_reply.started":"2022-03-16T20:08:29.292418Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"]},{"data":{"text/html":["Finishing last run (ID:lg5m8uy4) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ec3badca745c492b8feaa3ffeb5ad03c","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>close_generator</td><td>█▇▅▄▆▅▃▅▄▂▂▂▂▃▂▃▃▁▂█▃▂▅▃▂▆▅▃▄▂▂▂▃▁▂▁▂▂▃▃</td></tr><tr><td>d_loss</td><td>▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▅▅▁▄▃▄▄▄▄▄▄▄▄▅█▄█▂▄▅▄</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>fake_discriminator</td><td>▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▇▇▇▆▇▇▇█▆█▅▄▁▃▄▇▆▇</td></tr><tr><td>fake_generator</td><td>▇█▇▇█▇█████████▇▇▇██▇▆█▇▇▆▇███▆█▆▅▁▄▅█▆▇</td></tr><tr><td>g_loss</td><td>▇█▇▇█▇█████████▇▇▇██▇▆█▇▇▆▇███▆█▆▅▁▄▅█▆▇</td></tr><tr><td>penalty_discriminator</td><td>▂▃▂▁▂▂▃▁▂▁▃▂▂▂▁▁▁▁▂▁▂█▁▁▁▂▂▂▁▂▂▂▃▃▂▃▄▂▃▂</td></tr><tr><td>real_discriminator</td><td>▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▅▇▆▇▆▇▇██▆█▅▅▁▄▃█▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>close_generator</td><td>0.01501</td></tr><tr><td>d_loss</td><td>0.59336</td></tr><tr><td>epoch</td><td>180</td></tr><tr><td>fake_discriminator</td><td>-2.49327</td></tr><tr><td>fake_generator</td><td>-3.67856</td></tr><tr><td>g_loss</td><td>-3.67106</td></tr><tr><td>penalty_discriminator</td><td>0.0063</td></tr><tr><td>real_discriminator</td><td>-1.96292</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">easy-feather-13</strong>: <a href=\"https://wandb.ai/matiasetcheverry/crypto-prediction/runs/lg5m8uy4\" target=\"_blank\">https://wandb.ai/matiasetcheverry/crypto-prediction/runs/lg5m8uy4</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20220316_213641-lg5m8uy4/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:lg5m8uy4). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2022-03-16 22:51:47.047660: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n","2022-03-16 22:51:47.047741: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"]},{"data":{"text/html":["Tracking run with wandb version 0.12.11"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/matias/crypto-prediction/wandb/run-20220316_225138-gj6sx25c</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/matiasetcheverry/crypto-prediction/runs/gj6sx25c\" target=\"_blank\">ruby-moon-16</a></strong> to <a href=\"https://wandb.ai/matiasetcheverry/crypto-prediction\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["if log_wandb:\n","    import wandb\n","\n","    wandb_api.login()\n","    run = wandb.init(\n","        project=\"crypto-prediction\",\n","        group=\"Initial Gan\",\n","        job_type=\"test\",\n","    )\n"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2022-03-16T20:34:36.154852Z","iopub.status.busy":"2022-03-16T20:34:36.154520Z","iopub.status.idle":"2022-03-16T20:34:37.974419Z","shell.execute_reply":"2022-03-16T20:34:37.973401Z","shell.execute_reply.started":"2022-03-16T20:34:36.154819Z"},"trusted":true},"outputs":[],"source":["class DataModule:\n","    def __init__(\n","        self,\n","        config,\n","        csv_file=None,\n","        train_df=None,\n","        test_df=None,\n","        train_dataset=None,\n","        validation_dataset=None,\n","    ):\n","        super().__init__()\n","        self.config = config\n","\n","        if csv_file is not None:\n","            self.df = pd.read_csv(csv_file, delimiter=\";\")\n","            self.df[\"BEGINNING_DATE\"] = pd.to_datetime(\n","                self.df[\"BEGINNING_DATE\"], dayfirst=True\n","            )\n","            self.df[\"ENDING_DATE\"] = pd.to_datetime(\n","                self.df[\"ENDING_DATE\"], dayfirst=True\n","            )\n","            self.df[\"TICKER\"] += \"-USD\"\n","\n","        self.train_df = train_df.convert_dtypes() if train_df is not None else None\n","        self.test_df = test_df.convert_dtypes() if test_df is not None else None\n","        self.train_dataset = train_dataset\n","        self.validation_dataset = validation_dataset\n","\n","    def _preprocess_klines(\n","        self,\n","        data=None,\n","        ticker=None,\n","        beginning_date=None,\n","        ending_date=None,\n","        interval=\"1d\",\n","    ):\n","        if data is None:\n","            data = get_data.select_data(\n","                ticker,\n","                interval,\n","                beginning_date=beginning_date,\n","                ending_date=ending_date,\n","            )\n","\n","        data = data.dropna(axis=0)\n","        data = data.drop(labels=\"Date\", axis=1)\n","        data = data.replace(\n","            to_replace=[np.inf, -np.inf, np.float64(\"inf\"), -np.float64(\"inf\")],\n","            value=0,\n","        )\n","        idx_close = list(data.columns).index(\"Close\")\n","        scaler = MinMaxScaler(feature_range=(-1, 1))\n","        klines = tf.convert_to_tensor(scaler.fit_transform(data), dtype=tf.float32)\n","        data_close = klines[:, idx_close]\n","        single_close = tf.convert_to_tensor(\n","            data_close[self.config[\"nb_previous_close\"] :]\n","        )\n","        multiple_close = tf.stack(\n","            [\n","                data_close[i : i + self.config[\"nb_previous_close\"]]\n","                for i in range(len(data_close) - self.config[\"nb_previous_close\"])\n","            ]\n","        )\n","        multiple_klines = tf.stack(\n","            [\n","                klines[i : i + self.config[\"nb_previous_close\"], :]\n","                for i in range(len(klines) - self.config[\"nb_previous_close\"])\n","            ]\n","        )\n","        return multiple_klines, single_close, multiple_close\n","\n","    def setup(self):\n","        klines = []\n","        single_closes = []\n","        multiple_closes = []\n","        for index, row in self.df.iterrows():\n","            kline, single_close, multiple_close = self._preprocess_klines(\n","                ticker=row[\"TICKER\"],\n","                beginning_date=row[\"BEGINNING_DATE\"],\n","                ending_date=row[\"ENDING_DATE\"],\n","            )\n","            if len(kline) > 0:\n","                klines.append(kline)\n","                single_closes.append(single_close)\n","                multiple_closes.append(multiple_close)\n","\n","        self.klines = tf.data.Dataset.from_tensor_slices(\n","            tf.concat(klines, axis=0),\n","        )\n","        self.single_closes = tf.data.Dataset.from_tensor_slices(\n","            tf.expand_dims(\n","                (tf.expand_dims(tf.concat(single_closes, axis=0), axis=-1)), axis=-1\n","            ),\n","        )\n","        self.multiple_closes = tf.data.Dataset.from_tensor_slices(\n","            tf.expand_dims(tf.concat(multiple_closes, axis=0), axis=-1),\n","        )\n","        dataset = (\n","            tf.data.Dataset.zip((self.klines, self.multiple_closes, self.single_closes))\n","            .shuffle(len(self.klines), reshuffle_each_iteration=True)\n","            .batch(\n","                self.config[\"batch_size\"],\n","                drop_remainder=False,\n","                num_parallel_calls=tf.data.AUTOTUNE,\n","            )\n","        )\n","\n","        train_size = int(config[\"train_test_split\"] * len(dataset))\n","        self.train_dataset = dataset.take(train_size)\n","        self.val_dataset = dataset.skip(train_size)\n","\n","\n","dm = DataModule(config, repo_path / \"DATE.csv\")\n","dm.setup()\n","train_dataset = dm.train_dataset\n"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2022-03-16T20:34:37.976741Z","iopub.status.busy":"2022-03-16T20:34:37.976263Z","iopub.status.idle":"2022-03-16T20:35:22.149983Z","shell.execute_reply":"2022-03-16T20:35:22.149039Z","shell.execute_reply.started":"2022-03-16T20:34:37.976675Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/250\n","230/230 [==============================] - 35s 93ms/step - fake_discriminator: 0.6570 - real_discriminator: 0.7063 - penalty_discriminator: 0.0831 - d_loss: 0.7821 - fake_generator: 0.6566 - close_generator: 0.2809 - g_loss: -0.5161 - _timestamp: 1647467548.0000 - _runtime: 43.0000\n","Epoch 2/250\n","230/230 [==============================] - 21s 89ms/step - fake_discriminator: 0.7358 - real_discriminator: 0.9012 - penalty_discriminator: 0.0196 - d_loss: 0.0305 - fake_generator: 0.7359 - close_generator: 0.1780 - g_loss: -0.6469 - _timestamp: 1647467569.0000 - _runtime: 64.0000\n","Epoch 3/250\n","230/230 [==============================] - 20s 85ms/step - fake_discriminator: 0.6172 - real_discriminator: 0.7345 - penalty_discriminator: 0.0136 - d_loss: 0.0185 - fake_generator: 0.6111 - close_generator: 0.1274 - g_loss: -0.5474 - _timestamp: 1647467589.0000 - _runtime: 84.0000\n","Epoch 4/250\n","230/230 [==============================] - 22s 94ms/step - fake_discriminator: 0.4101 - real_discriminator: 0.5012 - penalty_discriminator: 0.0107 - d_loss: 0.0163 - fake_generator: 0.4042 - close_generator: 0.0992 - g_loss: -0.3546 - _timestamp: 1647467610.0000 - _runtime: 105.0000\n","Epoch 5/250\n","230/230 [==============================] - 20s 87ms/step - fake_discriminator: 0.4868 - real_discriminator: 0.5609 - penalty_discriminator: 0.0094 - d_loss: 0.0198 - fake_generator: 0.4920 - close_generator: 0.0785 - g_loss: -0.4528 - _timestamp: 1647467630.0000 - _runtime: 125.0000\n","Epoch 6/250\n","230/230 [==============================] - 20s 87ms/step - fake_discriminator: 0.4600 - real_discriminator: 0.5314 - penalty_discriminator: 0.0081 - d_loss: 0.0094 - fake_generator: 0.4508 - close_generator: 0.0713 - g_loss: -0.4152 - _timestamp: 1647467650.0000 - _runtime: 145.0000\n","Epoch 7/250\n","230/230 [==============================] - 20s 86ms/step - fake_discriminator: 0.4744 - real_discriminator: 0.5159 - penalty_discriminator: 0.0073 - d_loss: 0.0317 - fake_generator: 0.4570 - close_generator: 0.0614 - g_loss: -0.4263 - _timestamp: 1647467670.0000 - _runtime: 165.0000\n","Epoch 8/250\n","230/230 [==============================] - 20s 87ms/step - fake_discriminator: 0.3907 - real_discriminator: 0.4396 - penalty_discriminator: 0.0071 - d_loss: 0.0217 - fake_generator: 0.4023 - close_generator: 0.0561 - g_loss: -0.3743 - _timestamp: 1647467690.0000 - _runtime: 185.0000\n","Epoch 9/250\n","230/230 [==============================] - 19s 84ms/step - fake_discriminator: 0.5412 - real_discriminator: 0.5916 - penalty_discriminator: 0.0062 - d_loss: 0.0115 - fake_generator: 0.5399 - close_generator: 0.0521 - g_loss: -0.5138 - _timestamp: 1647467710.0000 - _runtime: 205.0000\n","Epoch 10/250\n","230/230 [==============================] - 21s 93ms/step - fake_discriminator: 0.7893 - real_discriminator: 0.8335 - penalty_discriminator: 0.0057 - d_loss: 0.0125 - fake_generator: 0.8009 - close_generator: 0.0496 - g_loss: -0.7761 - _timestamp: 1647467731.0000 - _runtime: 226.0000\n","Epoch 11/250\n","230/230 [==============================] - 23s 98ms/step - fake_discriminator: 0.6854 - real_discriminator: 0.7257 - penalty_discriminator: 0.0053 - d_loss: 0.0123 - fake_generator: 0.6912 - close_generator: 0.0466 - g_loss: -0.6679 - _timestamp: 1647467754.0000 - _runtime: 249.0000\n","Epoch 12/250\n","230/230 [==============================] - 21s 92ms/step - fake_discriminator: 0.2309 - real_discriminator: 0.2716 - penalty_discriminator: 0.0051 - d_loss: 0.0106 - fake_generator: 0.2163 - close_generator: 0.0460 - g_loss: -0.1933 - _timestamp: 1647467775.0000 - _runtime: 270.0000\n","Epoch 13/250\n","230/230 [==============================] - 21s 92ms/step - fake_discriminator: 0.2631 - real_discriminator: 0.2920 - penalty_discriminator: 0.0049 - d_loss: 0.0199 - fake_generator: 0.2620 - close_generator: 0.0420 - g_loss: -0.2410 - _timestamp: 1647467797.0000 - _runtime: 292.0000\n","Epoch 14/250\n","230/230 [==============================] - 20s 87ms/step - fake_discriminator: 0.5404 - real_discriminator: 0.5746 - penalty_discriminator: 0.0046 - d_loss: 0.0114 - fake_generator: 0.5506 - close_generator: 0.0409 - g_loss: -0.5302 - _timestamp: 1647467817.0000 - _runtime: 312.0000\n","Epoch 15/250\n","230/230 [==============================] - 19s 85ms/step - fake_discriminator: 0.5213 - real_discriminator: 0.5451 - penalty_discriminator: 0.0045 - d_loss: 0.0211 - fake_generator: 0.5031 - close_generator: 0.0402 - g_loss: -0.4830 - _timestamp: 1647467836.0000 - _runtime: 331.0000\n","Epoch 16/250\n","230/230 [==============================] - 20s 85ms/step - fake_discriminator: 0.2388 - real_discriminator: 0.2780 - penalty_discriminator: 0.0046 - d_loss: 0.0064 - fake_generator: 0.2617 - close_generator: 0.0375 - g_loss: -0.2430 - _timestamp: 1647467856.0000 - _runtime: 351.0000\n","Epoch 17/250\n","230/230 [==============================] - 20s 85ms/step - fake_discriminator: 0.3341 - real_discriminator: 0.3652 - penalty_discriminator: 0.0041 - d_loss: 0.0101 - fake_generator: 0.3302 - close_generator: 0.0376 - g_loss: -0.3114 - _timestamp: 1647467875.0000 - _runtime: 370.0000\n","Epoch 18/250\n","230/230 [==============================] - 20s 85ms/step - fake_discriminator: 0.2094 - real_discriminator: 0.2417 - penalty_discriminator: 0.0040 - d_loss: 0.0080 - fake_generator: 0.2168 - close_generator: 0.0365 - g_loss: -0.1985 - _timestamp: 1647467895.0000 - _runtime: 390.0000\n","Epoch 19/250\n","230/230 [==============================] - 20s 85ms/step - fake_discriminator: 0.4345 - real_discriminator: 0.4736 - penalty_discriminator: 0.0040 - d_loss: 8.5999e-04 - fake_generator: 0.4464 - close_generator: 0.0341 - g_loss: -0.4294 - _timestamp: 1647467915.0000 - _runtime: 410.0000\n","Epoch 20/250\n","230/230 [==============================] - 20s 85ms/step - fake_discriminator: 0.5008 - real_discriminator: 0.5265 - penalty_discriminator: 0.0039 - d_loss: 0.0134 - fake_generator: 0.4863 - close_generator: 0.0350 - g_loss: -0.4688 - _timestamp: 1647467934.0000 - _runtime: 429.0000\n","Epoch 21/250\n","230/230 [==============================] - 19s 84ms/step - fake_discriminator: 0.6205 - real_discriminator: 0.6450 - penalty_discriminator: 0.0044 - d_loss: 0.0190 - fake_generator: 0.6116 - close_generator: 0.0334 - g_loss: -0.5949 - _timestamp: 1647467954.0000 - _runtime: 449.0000\n","Epoch 22/250\n","230/230 [==============================] - 20s 85ms/step - fake_discriminator: 0.5823 - real_discriminator: 0.6125 - penalty_discriminator: 0.0042 - d_loss: 0.0121 - fake_generator: 0.5807 - close_generator: 0.0336 - g_loss: -0.5639 - _timestamp: 1647467973.0000 - _runtime: 468.0000\n","Epoch 23/250\n","230/230 [==============================] - 20s 87ms/step - fake_discriminator: 0.5535 - real_discriminator: 0.5921 - penalty_discriminator: 0.0038 - d_loss: -0.0010 - fake_generator: 0.5703 - close_generator: 0.0328 - g_loss: -0.5539 - _timestamp: 1647467993.0000 - _runtime: 488.0000\n","Epoch 24/250\n","230/230 [==============================] - 22s 98ms/step - fake_discriminator: 0.5314 - real_discriminator: 0.5532 - penalty_discriminator: 0.0036 - d_loss: 0.0146 - fake_generator: 0.5165 - close_generator: 0.0315 - g_loss: -0.5007 - _timestamp: 1647468016.0000 - _runtime: 511.0000\n","Epoch 25/250\n","230/230 [==============================] - 22s 97ms/step - fake_discriminator: 0.6827 - real_discriminator: 0.7157 - penalty_discriminator: 0.0034 - d_loss: 0.0012 - fake_generator: 0.6789 - close_generator: 0.0308 - g_loss: -0.6636 - _timestamp: 1647468038.0000 - _runtime: 533.0000\n","Epoch 26/250\n","230/230 [==============================] - 24s 106ms/step - fake_discriminator: 0.4339 - real_discriminator: 0.4595 - penalty_discriminator: 0.0033 - d_loss: 0.0070 - fake_generator: 0.4251 - close_generator: 0.0298 - g_loss: -0.4102 - _timestamp: 1647468063.0000 - _runtime: 558.0000\n","Epoch 27/250\n","230/230 [==============================] - 21s 89ms/step - fake_discriminator: 0.2851 - real_discriminator: 0.2930 - penalty_discriminator: 0.0036 - d_loss: 0.0277 - fake_generator: 0.2812 - close_generator: 0.0295 - g_loss: -0.2664 - _timestamp: 1647468083.0000 - _runtime: 578.0000\n","Epoch 28/250\n","230/230 [==============================] - 19s 83ms/step - fake_discriminator: 0.3812 - real_discriminator: 0.4091 - penalty_discriminator: 0.0036 - d_loss: 0.0084 - fake_generator: 0.3831 - close_generator: 0.0292 - g_loss: -0.3685 - _timestamp: 1647468102.0000 - _runtime: 597.0000\n","Epoch 29/250\n","230/230 [==============================] - 23s 101ms/step - fake_discriminator: 0.5000 - real_discriminator: 0.5271 - penalty_discriminator: 0.0035 - d_loss: 0.0079 - fake_generator: 0.5068 - close_generator: 0.0284 - g_loss: -0.4926 - _timestamp: 1647468126.0000 - _runtime: 621.0000\n","Epoch 30/250\n","230/230 [==============================] - 21s 93ms/step - fake_discriminator: 0.6292 - real_discriminator: 0.6468 - penalty_discriminator: 0.0034 - d_loss: 0.0162 - fake_generator: 0.6352 - close_generator: 0.0281 - g_loss: -0.6211 - _timestamp: 1647468147.0000 - _runtime: 642.0000\n","Epoch 31/250\n","230/230 [==============================] - 20s 87ms/step - fake_discriminator: 0.6670 - real_discriminator: 0.6904 - penalty_discriminator: 0.0035 - d_loss: 0.0113 - fake_generator: 0.6761 - close_generator: 0.0268 - g_loss: -0.6627 - _timestamp: 1647468167.0000 - _runtime: 662.0000\n","Epoch 32/250\n","230/230 [==============================] - 23s 98ms/step - fake_discriminator: 0.9460 - real_discriminator: 0.9742 - penalty_discriminator: 0.0034 - d_loss: 0.0058 - fake_generator: 0.9396 - close_generator: 0.0278 - g_loss: -0.9258 - _timestamp: 1647468190.0000 - _runtime: 685.0000\n","Epoch 33/250\n","230/230 [==============================] - 21s 90ms/step - fake_discriminator: 1.1873 - real_discriminator: 1.2153 - penalty_discriminator: 0.0032 - d_loss: 0.0044 - fake_generator: 1.1991 - close_generator: 0.0265 - g_loss: -1.1859 - _timestamp: 1647468211.0000 - _runtime: 706.0000\n","Epoch 34/250\n","230/230 [==============================] - 21s 91ms/step - fake_discriminator: 1.3841 - real_discriminator: 1.4284 - penalty_discriminator: 0.0031 - d_loss: -0.0134 - fake_generator: 1.3821 - close_generator: 0.0271 - g_loss: -1.3685 - _timestamp: 1647468232.0000 - _runtime: 727.0000\n","Epoch 35/250\n","230/230 [==============================] - 24s 103ms/step - fake_discriminator: 1.5352 - real_discriminator: 1.5755 - penalty_discriminator: 0.0030 - d_loss: -0.0102 - fake_generator: 1.5453 - close_generator: 0.0261 - g_loss: -1.5322 - _timestamp: 1647468255.0000 - _runtime: 750.0000\n","Epoch 36/250\n","230/230 [==============================] - 23s 99ms/step - fake_discriminator: 1.6347 - real_discriminator: 1.6566 - penalty_discriminator: 0.0030 - d_loss: 0.0078 - fake_generator: 1.6407 - close_generator: 0.0253 - g_loss: -1.6280 - _timestamp: 1647468278.0000 - _runtime: 773.0000\n","Epoch 37/250\n","162/230 [====================>.........] - ETA: 6s - fake_discriminator: 1.6766 - real_discriminator: 1.7132 - penalty_discriminator: 0.0028 - d_loss: -0.0081 - fake_generator: 1.7059 - close_generator: 0.0251 - g_loss: -1.6934"]}],"source":["from __future__ import division, print_function\n","\n","import sys\n","from functools import partial\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from tensorflow.keras import layers\n","from tensorflow.keras.activations import sigmoid, tanh\n","import wandb\n","from wandb.keras import WandbCallback\n","from tensorflow.keras.models import Model\n","\n","\n","class WGANGP(Model):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","        # Following parameter and optimizer set as recommended in paper\n","        self.n_critic = 5\n","\n","        # Build the generator and critic\n","        self.generator = self.build_generator()\n","        self.discriminator = self.build_discriminator()\n","\n","        generator_optimizer = tf.keras.optimizers.Adam(\n","            learning_rate=self.config[\"learning_rate_generator\"],\n","            beta_1=self.config[\"beta1\"],\n","            beta_2=self.config[\"beta2\"],\n","        )\n","        discriminator_optimizer = tf.keras.optimizers.Adam(\n","            learning_rate=self.config[\"learning_rate_discriminator\"],\n","            beta_1=self.config[\"beta1\"],\n","            beta_2=self.config[\"beta2\"],\n","        )\n","\n","        self.compile(\n","            d_optimizer=discriminator_optimizer,\n","            g_optimizer=generator_optimizer,\n","            g_loss_fn=self.generator_loss,\n","            d_loss_fn=self.discriminator_loss,\n","        )\n","\n","    def build_generator(self):\n","        inputs = layers.Input(\n","            shape=(self.config[\"nb_previous_close\"], 29),\n","            batch_size=self.config[\"batch_size\"],\n","        )\n","        outputs = layers.Conv1D(32, 3, activation=layers.LeakyReLU(alpha=0.1))(inputs)\n","        outputs = layers.Bidirectional(\n","            layers.LSTM(64, dropout=0.3, activation=layers.ReLU())\n","        )(outputs)\n","        outputs = layers.Flatten()(outputs)\n","        outputs = layers.Dense(64, activation=layers.LeakyReLU(alpha=0.1))(outputs)\n","        outputs = layers.Dropout(0.2)(outputs)\n","        outputs = layers.Dense(32, activation=layers.LeakyReLU(alpha=0.1))(outputs)\n","        outputs = layers.Dropout(0.2)(outputs)\n","        outputs = layers.Dense(1, activation=None)(outputs)\n","        outputs = layers.Reshape(target_shape=(1, 1))(outputs)\n","        generator = Model(inputs=inputs, outputs=outputs, name=\"genrator\")\n","        return generator\n","\n","    def build_discriminator(self):\n","        inputs = layers.Input(\n","            shape=(self.config[\"nb_previous_close\"] + 1, 1),\n","            batch_size=self.config[\"batch_size\"],\n","        )\n","        outputs = layers.Conv1D(32, 3, activation=layers.LeakyReLU(alpha=0.1))(inputs)\n","        outputs = layers.Conv1D(64, 3, activation=layers.LeakyReLU(alpha=0.1))(outputs)\n","        outputs = layers.Flatten()(outputs)\n","        outputs = layers.Dense(64, activation=layers.LeakyReLU(alpha=0.1))(outputs)\n","        outputs = layers.Dropout(0.2)(outputs)\n","        outputs = layers.Dense(32, activation=layers.LeakyReLU(alpha=0.1))(outputs)\n","        outputs = layers.Dropout(0.2)(outputs)\n","        outputs = layers.Dense(1, activation=None)(outputs)\n","        discriminator = Model(inputs=inputs, outputs=outputs, name=\"discriminator\")\n","        return discriminator\n","\n","    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n","        super().compile()\n","        self.d_optimizer = d_optimizer\n","        self.g_optimizer = g_optimizer\n","        self.d_loss_fn = d_loss_fn\n","        self.g_loss_fn = g_loss_fn\n","\n","    def discriminator_loss(self, real, fake, current_metrics={}):\n","        real_loss = tf.reduce_mean(real)\n","        fake_loss = tf.reduce_mean(fake)\n","\n","        current_metrics[\"real_discriminator\"] = real_loss\n","        current_metrics[\"fake_discriminator\"] = fake_loss\n","        return fake_loss - real_loss\n","\n","    # Define the loss functions for the generator.\n","    def generator_loss(self, fake, real, fake_close, real_close, current_metrics={}):\n","        fake_loss = tf.reduce_mean(fake)\n","        far_loss = tf.keras.metrics.mean_squared_error(\n","            tf.squeeze(real_close), tf.squeeze(fake_close)\n","        )\n","        g_loss = -fake_loss + 0.5 * far_loss\n","        current_metrics[\"fake_generator\"] = fake_loss\n","        current_metrics[\"close_generator\"] = far_loss\n","        current_metrics[\"g_loss\"] = g_loss\n","        return g_loss\n","\n","    def gradient_penalty(self, real_images, fake_images, current_metrics={}):\n","        \"\"\"Calculates the gradient penalty.\n","\n","        This loss is calculated on an interpolated image\n","        and added to the discriminator loss.\n","        \"\"\"\n","        alpha = tf.random.normal((self.config[\"batch_size\"], 1), 0.0, 1.0)\n","        diff = fake_images - real_images\n","\n","        interpolated = real_images + tf.multiply(diff, alpha[:, tf.newaxis])\n","\n","        with tf.GradientTape() as gp_tape:\n","            gp_tape.watch(interpolated)\n","            pred = self.discriminator(interpolated, training=True)\n","\n","        grads = gp_tape.gradient(pred, [interpolated])[0]\n","        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2]))\n","        gp = tf.reduce_mean((norm - 1.0) ** 2)\n","\n","        current_metrics[\"penalty_discriminator\"] = gp\n","        return gp\n","\n","    def update_metrics(self, metrics, current_metrics, reduction=None):\n","        if reduction == \"mean\":\n","            factor_reduction = self.config[\"n_discriminator\"]\n","        else:\n","            factor_reduction = 1\n","\n","        for name, value in current_metrics.items():\n","            metrics[name] = metrics.get(name, 0) + value / factor_reduction\n","\n","    def train_step(self, data):\n","        klines, previous_closes, real_close = data\n","\n","        metrics = {\n","            \"fake_discriminator\": 0,\n","            \"real_discriminator\": 0,\n","            \"penalty_discriminator\": 0,\n","            \"d_loss\": 0,\n","            \"fake_generator\": 0,\n","            \"close_generator\": 0,\n","            \"g_loss\": 0,\n","        }\n","        for _ in range(self.config[\"n_discriminator\"]):\n","            current_metrics = {}\n","            with tf.GradientTape() as tape:\n","                fake_close = self.generator(klines, training=True)\n","                fake_closes = tf.concat(\n","                    [previous_closes, fake_close],\n","                    axis=1,\n","                )\n","                real_closes = tf.concat([previous_closes, real_close], axis=1)\n","                fake = self.discriminator(fake_closes, training=True)\n","                real = self.discriminator(real_closes, training=True)\n","                d_cost = self.d_loss_fn(real, fake, current_metrics)\n","                gp = self.gradient_penalty(real_closes, fake_closes, current_metrics)\n","                d_loss = d_cost + gp * self.config[\"gp_weight\"]\n","                current_metrics[\"d_loss\"] = d_loss\n","\n","            self.update_metrics(metrics, current_metrics, reduction=\"mean\")\n","\n","            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n","            self.d_optimizer.apply_gradients(\n","                zip(d_gradient, self.discriminator.trainable_variables)\n","            )\n","\n","        with tf.GradientTape() as tape:\n","            current_metrics = {}\n","            fake_close = self.generator(klines, training=True)\n","            fake_closes = tf.concat(\n","                [previous_closes, fake_close],\n","                axis=1,\n","            )\n","            real_closes = tf.concat([previous_closes, real_close], axis=1)\n","            real = self.discriminator(real_closes, training=True)\n","            fake = self.discriminator(fake_closes, training=True)\n","            g_loss = self.g_loss_fn(fake, real, fake_close, real_close, current_metrics)\n","        self.update_metrics(metrics, current_metrics)\n","        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n","\n","        self.g_optimizer.apply_gradients(\n","            zip(gen_gradient, self.generator.trainable_variables)\n","        )\n","        return metrics\n","\n","\n","wgan = WGANGP(config)\n","\n","# Instantiate the optimizer for both networks\n","# (learning_rate=0.0002, beta_1=0.5 are recommended\n","\n","# Define the loss functions for the discriminator,\n","# which should be (fake_loss - real_loss).\n","# We will add the gradient penalty later to this loss function.\n","\n","\n","# Set the number of epochs for trainining.\n","epochs = 250\n","\n","\n","# Start training the model.\n","wgan.fit(dm.train_dataset, epochs=epochs, callbacks=[WandbCallback()])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["run.finish()\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.8"}},"nbformat":4,"nbformat_minor":4}
