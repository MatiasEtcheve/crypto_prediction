{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 15:36:01.749037: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-16 15:36:01.749081: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-03-16 15:36:04.056284: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-03-16 15:36:04.056319: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-16 15:36:04.056332: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Matias): /proc/driver/nvidia/version does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'CPU'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "if IS_KAGGLE:\n",
    "    repo_path = Path(\"../input/crypto-prediction\")\n",
    "elif IS_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "    repo_path = Path(\"/content/gdrive/MyDrive/crypto-prediction\")\n",
    "else:\n",
    "    repo_path = Path(\"/home/matias/crypto-prediction\")\n",
    "sys.path.append(str(repo_path))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import tensorflow as tf\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torchviz import make_dot\n",
    "\n",
    "import get_data\n",
    "import wandb\n",
    "from tools import dataframe_reformat, inspect_code, plotting, training, wandb_api\n",
    "\n",
    "log_wandb = True\n",
    "tf.config.list_physical_devices(\"CPU\")[0].device_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatiasetcheverry\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/matias/.netrc\n",
      "2022-03-16 15:36:06.234180: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-16 15:36:06.234221: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/matias/crypto-prediction/wandb/run-20220316_153604-a1o5vgku</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/matiasetcheverry/crypto-prediction/runs/a1o5vgku\" target=\"_blank\">leafy-dream-5</a></strong> to <a href=\"https://wandb.ai/matiasetcheverry/crypto-prediction\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if log_wandb:\n",
    "    import wandb\n",
    "\n",
    "    wandb_api.login()\n",
    "    run = wandb.init(\n",
    "        project=\"crypto-prediction\",\n",
    "        group=\"Initial Gan\",\n",
    "        job_type=\"test\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if log_wandb:\n",
    "    config = wandb.config\n",
    "else:\n",
    "    config = {}\n",
    "\n",
    "config[\"job_type\"] = run.job_type if \"run\" in locals() else \"test\"\n",
    "config[\"log_wandb\"] = False\n",
    "config[\"train_test_split\"] = 0.7\n",
    "config[\"nb_previous_close\"] = 2\n",
    "config[\"batch_size\"] = 32\n",
    "config[\"n_discriminator\"] = 50\n",
    "config[\"gp_weight\"] = 10\n",
    "config[\"learning_rate_generator\"] = 0.0001\n",
    "config[\"learning_rate_discriminator\"] = 0.0004\n",
    "config[\"beta1\"] = 0.5\n",
    "config[\"beta2\"] = 0.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 15:36:08.383142: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264 114\n",
      "(32, 2, 29) (32, 2, 1) (32, 1, 1)\n",
      "<TakeDataset element_spec=(TensorSpec(shape=(None, 2, 29), dtype=tf.float32, name=None), TensorSpec(shape=(None, 2, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1, 1), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "class DataModule:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        csv_file=None,\n",
    "        train_df=None,\n",
    "        test_df=None,\n",
    "        train_dataset=None,\n",
    "        validation_dataset=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        if csv_file is not None:\n",
    "            self.df = pd.read_csv(csv_file, delimiter=\";\")\n",
    "            self.df[\"BEGINNING_DATE\"] = pd.to_datetime(\n",
    "                self.df[\"BEGINNING_DATE\"], dayfirst=True\n",
    "            )\n",
    "            self.df[\"ENDING_DATE\"] = pd.to_datetime(\n",
    "                self.df[\"ENDING_DATE\"], dayfirst=True\n",
    "            )\n",
    "            self.df[\"TICKER\"] += \"-USD\"\n",
    "\n",
    "        self.train_df = train_df.convert_dtypes() if train_df is not None else None\n",
    "        self.test_df = test_df.convert_dtypes() if test_df is not None else None\n",
    "        self.train_dataset = train_dataset\n",
    "        self.validation_dataset = validation_dataset\n",
    "\n",
    "    def _preprocess_klines(\n",
    "        self,\n",
    "        data=None,\n",
    "        ticker=None,\n",
    "        beginning_date=None,\n",
    "        ending_date=None,\n",
    "        interval=\"1d\",\n",
    "    ):\n",
    "        if data is None:\n",
    "            data = get_data.select_data(\n",
    "                ticker,\n",
    "                interval,\n",
    "                beginning_date=beginning_date,\n",
    "                ending_date=ending_date,\n",
    "            )\n",
    "        data.dropna(axis=0, inplace=True)\n",
    "        data.drop(labels=\"Date\", axis=1, inplace=True)\n",
    "        data.replace(\n",
    "            to_replace=[np.inf, -np.inf, np.float64(\"inf\"), -np.float64(\"inf\")],\n",
    "            value=0,\n",
    "            inplace=True,\n",
    "        )\n",
    "        self.data = data\n",
    "        idx_close = list(data.columns).index(\"Close\")\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        klines = tf.convert_to_tensor(scaler.fit_transform(data))\n",
    "\n",
    "        data_close = klines[:, idx_close]\n",
    "        single_close = tf.convert_to_tensor(\n",
    "            data_close[self.config[\"nb_previous_close\"] :]\n",
    "        )\n",
    "        multiple_close = tf.stack(\n",
    "            [\n",
    "                data_close[i : i + self.config[\"nb_previous_close\"]]\n",
    "                for i in range(len(data_close) - self.config[\"nb_previous_close\"])\n",
    "            ]\n",
    "        )\n",
    "        multiple_klines = tf.stack(\n",
    "            [\n",
    "                klines[i : i + self.config[\"nb_previous_close\"], :]\n",
    "                for i in range(len(klines) - self.config[\"nb_previous_close\"])\n",
    "            ]\n",
    "        )\n",
    "        return multiple_klines, single_close, multiple_close\n",
    "\n",
    "    def prepare_data(self):\n",
    "        for _, row in self.df.iterrows():\n",
    "            _ = get_data.select_data(\n",
    "                row[\"TICKER\"],\n",
    "                \"1d\",\n",
    "                beginning_date=row[\"BEGINNING_DATE\"],\n",
    "                ending_date=row[\"ENDING_DATE\"],\n",
    "            )\n",
    "\n",
    "    def setup(self):\n",
    "        klines = []\n",
    "        single_closes = []\n",
    "        multiple_closes = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            kline, single_close, multiple_close = self._preprocess_klines(\n",
    "                ticker=row[\"TICKER\"],\n",
    "                beginning_date=row[\"BEGINNING_DATE\"],\n",
    "                ending_date=row[\"ENDING_DATE\"],\n",
    "            )\n",
    "            klines.append(kline)\n",
    "            single_closes.append(single_close)\n",
    "            multiple_closes.append(multiple_close)\n",
    "\n",
    "        klines = tf.data.Dataset.from_tensor_slices(\n",
    "            tf.cast(tf.concat(klines, axis=0), dtype=tf.float32)\n",
    "        )\n",
    "        single_closes = tf.data.Dataset.from_tensor_slices(\n",
    "            tf.cast(\n",
    "                tf.expand_dims(\n",
    "                    (tf.expand_dims(tf.concat(single_closes, axis=0), axis=-1)), axis=-1\n",
    "                ), \n",
    "                dtype=tf.float32)\n",
    "            \n",
    "        )\n",
    "        multiple_closes = tf.data.Dataset.from_tensor_slices(\n",
    "            tf.cast(tf.expand_dims(tf.concat(multiple_closes, axis=0), axis=-1), dtype=tf.float32,),\n",
    "            \n",
    "        )\n",
    "        dataset = (\n",
    "            tf.data.Dataset.zip((klines, multiple_closes, single_closes))\n",
    "            .shuffle(len(klines), reshuffle_each_iteration=True)\n",
    "            .batch(\n",
    "                self.config[\"batch_size\"],\n",
    "                drop_remainder=False,\n",
    "                num_parallel_calls=tf.data.AUTOTUNE,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        train_size = int(config[\"train_test_split\"] * len(dataset))\n",
    "        self.train_dataset = dataset.take(train_size)\n",
    "        self.val_dataset = dataset.skip(train_size)\n",
    "\n",
    "\n",
    "dm = DataModule(config, \"DATE.csv\")\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "print(len(dm.train_dataset), len(dm.val_dataset))\n",
    "train_dataset = dm.train_dataset\n",
    "print(\n",
    "    next(iter(train_dataset))[0].shape,\n",
    "    next(iter(train_dataset))[1].shape,\n",
    "    next(iter(train_dataset))[2].shape,\n",
    ")\n",
    "print(dm.train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "264/264 [==============================] - 15s 16ms/step - fake_discriminator: -0.0766 - real_discriminator: -0.0275 - penalty_discriminator: 0.0416 - d_loss: 0.3667 - fake_generator: 0.0779 - close_generator: 0.0229 - g_loss: 0.0893 - _timestamp: 1647441386.0000 - _runtime: 22.0000\n",
      "Epoch 2/300\n",
      "264/264 [==============================] - 5s 18ms/step - fake_discriminator: -0.0694 - real_discriminator: -0.0551 - penalty_discriminator: 0.0159 - d_loss: 0.1445 - fake_generator: 0.0674 - close_generator: 0.0196 - g_loss: 0.0772 - _timestamp: 1647441391.0000 - _runtime: 27.0000\n",
      "Epoch 3/300\n",
      "264/264 [==============================] - 4s 15ms/step - fake_discriminator: -0.1045 - real_discriminator: -0.0934 - penalty_discriminator: 0.0147 - d_loss: 0.1361 - fake_generator: 0.1061 - close_generator: 0.0505 - g_loss: 0.1314 - _timestamp: 1647441395.0000 - _runtime: 31.0000\n",
      "Epoch 4/300\n",
      "264/264 [==============================] - 4s 16ms/step - fake_discriminator: -0.0201 - real_discriminator: -0.0108 - penalty_discriminator: 0.0141 - d_loss: 0.1314 - fake_generator: 0.0162 - close_generator: 0.0799 - g_loss: 0.0562 - _timestamp: 1647441399.0000 - _runtime: 35.0000\n",
      "Epoch 5/300\n",
      "264/264 [==============================] - 4s 14ms/step - fake_discriminator: 0.1010 - real_discriminator: 0.1074 - penalty_discriminator: 0.0137 - d_loss: 0.1310 - fake_generator: -0.0998 - close_generator: 0.0866 - g_loss: -0.0565 - _timestamp: 1647441403.0000 - _runtime: 39.0000\n",
      "Epoch 6/300\n",
      "264/264 [==============================] - 5s 18ms/step - fake_discriminator: -0.0905 - real_discriminator: -0.0819 - penalty_discriminator: 0.0135 - d_loss: 0.1266 - fake_generator: 0.0908 - close_generator: 0.1093 - g_loss: 0.1454 - _timestamp: 1647441408.0000 - _runtime: 44.0000\n",
      "Epoch 7/300\n",
      "264/264 [==============================] - 5s 19ms/step - fake_discriminator: -0.3005 - real_discriminator: -0.2920 - penalty_discriminator: 0.0140 - d_loss: 0.1314 - fake_generator: 0.3016 - close_generator: 0.1229 - g_loss: 0.3631 - _timestamp: 1647441413.0000 - _runtime: 49.0000\n",
      "Epoch 8/300\n",
      "264/264 [==============================] - 4s 14ms/step - fake_discriminator: -0.2986 - real_discriminator: -0.2907 - penalty_discriminator: 0.0135 - d_loss: 0.1271 - fake_generator: 0.3026 - close_generator: 0.1626 - g_loss: 0.3839 - _timestamp: 1647441416.0000 - _runtime: 52.0000\n",
      "Epoch 9/300\n",
      "264/264 [==============================] - 5s 17ms/step - fake_discriminator: -0.4390 - real_discriminator: -0.4307 - penalty_discriminator: 0.0125 - d_loss: 0.1163 - fake_generator: 0.4387 - close_generator: 0.2042 - g_loss: 0.5408 - _timestamp: 1647441421.0000 - _runtime: 57.0000\n",
      "Epoch 10/300\n",
      "264/264 [==============================] - 4s 13ms/step - fake_discriminator: -0.5175 - real_discriminator: -0.5072 - penalty_discriminator: 0.0099 - d_loss: 0.0892 - fake_generator: 0.5108 - close_generator: 0.2560 - g_loss: 0.6388 - _timestamp: 1647441425.0000 - _runtime: 61.0000\n",
      "Epoch 11/300\n",
      "264/264 [==============================] - 3s 11ms/step - fake_discriminator: -0.5278 - real_discriminator: -0.5207 - penalty_discriminator: 0.0086 - d_loss: 0.0789 - fake_generator: 0.5366 - close_generator: 0.2660 - g_loss: 0.6697 - _timestamp: 1647441428.0000 - _runtime: 64.0000\n",
      "Epoch 12/300\n",
      "264/264 [==============================] - 4s 14ms/step - fake_discriminator: -0.5637 - real_discriminator: -0.5592 - penalty_discriminator: 0.0081 - d_loss: 0.0763 - fake_generator: 0.5706 - close_generator: 0.2235 - g_loss: 0.6824 - _timestamp: 1647441432.0000 - _runtime: 68.0000\n",
      "Epoch 13/300\n",
      "264/264 [==============================] - 3s 13ms/step - fake_discriminator: -0.5696 - real_discriminator: -0.5604 - penalty_discriminator: 0.0089 - d_loss: 0.0800 - fake_generator: 0.5751 - close_generator: 0.1707 - g_loss: 0.6604 - _timestamp: 1647441435.0000 - _runtime: 71.0000\n",
      "Epoch 14/300\n",
      "264/264 [==============================] - 4s 15ms/step - fake_discriminator: -0.2742 - real_discriminator: -0.2646 - penalty_discriminator: 0.0076 - d_loss: 0.0666 - fake_generator: 0.2721 - close_generator: 0.2934 - g_loss: 0.4188 - _timestamp: 1647441439.0000 - _runtime: 75.0000\n",
      "Epoch 15/300\n",
      "264/264 [==============================] - 4s 15ms/step - fake_discriminator: -0.3292 - real_discriminator: -0.3207 - penalty_discriminator: 0.0071 - d_loss: 0.0627 - fake_generator: 0.3307 - close_generator: 0.3376 - g_loss: 0.4995 - _timestamp: 1647441443.0000 - _runtime: 79.0000\n",
      "Epoch 16/300\n",
      "264/264 [==============================] - 4s 14ms/step - fake_discriminator: -0.4496 - real_discriminator: -0.4465 - penalty_discriminator: 0.0115 - d_loss: 0.1119 - fake_generator: 0.4498 - close_generator: 0.1512 - g_loss: 0.5254 - _timestamp: 1647441447.0000 - _runtime: 83.0000\n",
      "Epoch 17/300\n",
      "264/264 [==============================] - 3s 12ms/step - fake_discriminator: -0.2762 - real_discriminator: -0.2675 - penalty_discriminator: 0.0109 - d_loss: 0.1001 - fake_generator: 0.2796 - close_generator: 0.1579 - g_loss: 0.3586 - _timestamp: 1647441450.0000 - _runtime: 86.0000\n",
      "Epoch 18/300\n",
      "264/264 [==============================] - 4s 14ms/step - fake_discriminator: 0.1380 - real_discriminator: 0.1485 - penalty_discriminator: 0.0080 - d_loss: 0.0698 - fake_generator: -0.1385 - close_generator: 0.4223 - g_loss: 0.0726 - _timestamp: 1647441453.0000 - _runtime: 89.0000\n",
      "Epoch 19/300\n",
      "264/264 [==============================] - 4s 14ms/step - fake_discriminator: 0.2958 - real_discriminator: 0.3008 - penalty_discriminator: 0.0070 - d_loss: 0.0645 - fake_generator: -0.2976 - close_generator: 0.4729 - g_loss: -0.0611 - _timestamp: 1647441457.0000 - _runtime: 93.0000\n",
      "Epoch 20/300\n",
      "264/264 [==============================] - 4s 14ms/step - fake_discriminator: 0.5111 - real_discriminator: 0.5147 - penalty_discriminator: 0.0103 - d_loss: 0.0994 - fake_generator: -0.5094 - close_generator: 0.3622 - g_loss: -0.3283 - _timestamp: 1647441461.0000 - _runtime: 97.0000\n",
      "Epoch 21/300\n",
      "264/264 [==============================] - 3s 13ms/step - fake_discriminator: 0.5430 - real_discriminator: 0.5552 - penalty_discriminator: 0.0071 - d_loss: 0.0584 - fake_generator: -0.5317 - close_generator: 0.5787 - g_loss: -0.2424 - _timestamp: 1647441464.0000 - _runtime: 100.0000\n",
      "Epoch 22/300\n",
      "264/264 [==============================] - 4s 16ms/step - fake_discriminator: 0.6215 - real_discriminator: 0.6399 - penalty_discriminator: 0.0055 - d_loss: 0.0364 - fake_generator: -0.6355 - close_generator: 1.5327 - g_loss: 0.1309 - _timestamp: 1647441468.0000 - _runtime: 104.0000\n",
      "Epoch 23/300\n",
      "264/264 [==============================] - 4s 13ms/step - fake_discriminator: 0.9247 - real_discriminator: 0.9201 - penalty_discriminator: 0.0049 - d_loss: 0.0534 - fake_generator: -0.9170 - close_generator: 1.7632 - g_loss: -0.0354 - _timestamp: 1647441472.0000 - _runtime: 108.0000\n",
      "Epoch 24/300\n",
      "264/264 [==============================] - 3s 13ms/step - fake_discriminator: 0.8705 - real_discriminator: 0.8720 - penalty_discriminator: 0.0052 - d_loss: 0.0508 - fake_generator: -0.8654 - close_generator: 0.8327 - g_loss: -0.4490 - _timestamp: 1647441475.0000 - _runtime: 111.0000\n",
      "Epoch 25/300\n",
      "264/264 [==============================] - 3s 13ms/step - fake_discriminator: 0.9864 - real_discriminator: 0.9986 - penalty_discriminator: 0.0045 - d_loss: 0.0330 - fake_generator: -0.9895 - close_generator: 1.1644 - g_loss: -0.4073 - _timestamp: 1647441479.0000 - _runtime: 115.0000\n",
      "Epoch 26/300\n",
      "264/264 [==============================] - 3s 13ms/step - fake_discriminator: 1.2445 - real_discriminator: 1.2511 - penalty_discriminator: 0.0043 - d_loss: 0.0369 - fake_generator: -1.2297 - close_generator: 2.5846 - g_loss: 0.0626 - _timestamp: 1647441482.0000 - _runtime: 118.0000\n",
      "Epoch 27/300\n",
      "264/264 [==============================] - 3s 12ms/step - fake_discriminator: 1.2941 - real_discriminator: 1.3043 - penalty_discriminator: 0.0042 - d_loss: 0.0318 - fake_generator: -1.2815 - close_generator: 3.4864 - g_loss: 0.4617 - _timestamp: 1647441486.0000 - _runtime: 122.0000\n",
      "Epoch 28/300\n",
      "264/264 [==============================] - 4s 14ms/step - fake_discriminator: 1.3578 - real_discriminator: 1.3597 - penalty_discriminator: 0.0043 - d_loss: 0.0410 - fake_generator: -1.3582 - close_generator: 3.2205 - g_loss: 0.2520 - _timestamp: 1647441489.0000 - _runtime: 125.0000\n",
      "Epoch 29/300\n",
      "264/264 [==============================] - 3s 13ms/step - fake_discriminator: 1.2267 - real_discriminator: 1.2232 - penalty_discriminator: 0.0042 - d_loss: 0.0454 - fake_generator: -1.2222 - close_generator: 2.6489 - g_loss: 0.1022 - _timestamp: 1647441493.0000 - _runtime: 129.0000\n",
      "Epoch 30/300\n",
      "264/264 [==============================] - 3s 13ms/step - fake_discriminator: 1.2027 - real_discriminator: 1.2099 - penalty_discriminator: 0.0045 - d_loss: 0.0379 - fake_generator: -1.2156 - close_generator: 1.8426 - g_loss: -0.2943 - _timestamp: 1647441496.0000 - _runtime: 132.0000\n",
      "Epoch 31/300\n",
      "264/264 [==============================] - 3s 13ms/step - fake_discriminator: 1.0271 - real_discriminator: 1.0278 - penalty_discriminator: 0.0045 - d_loss: 0.0448 - fake_generator: -1.0347 - close_generator: 1.8720 - g_loss: -0.0987 - _timestamp: 1647441500.0000 - _runtime: 136.0000\n",
      "Epoch 32/300\n",
      "264/264 [==============================] - 3s 10ms/step - fake_discriminator: 0.8777 - real_discriminator: 0.8907 - penalty_discriminator: 0.0053 - d_loss: 0.0399 - fake_generator: -0.8826 - close_generator: 3.2532 - g_loss: 0.7441 - _timestamp: 1647441502.0000 - _runtime: 138.0000\n",
      "Epoch 33/300\n",
      "264/264 [==============================] - 3s 12ms/step - fake_discriminator: 0.8566 - real_discriminator: 0.8524 - penalty_discriminator: 0.0044 - d_loss: 0.0485 - fake_generator: -0.8515 - close_generator: 2.5550 - g_loss: 0.4260 - _timestamp: 1647441506.0000 - _runtime: 142.0000\n",
      "Epoch 34/300\n",
      "264/264 [==============================] - 2s 9ms/step - fake_discriminator: 1.0146 - real_discriminator: 1.0226 - penalty_discriminator: 0.0043 - d_loss: 0.0347 - fake_generator: -1.0299 - close_generator: 2.2529 - g_loss: 0.0966 - _timestamp: 1647441508.0000 - _runtime: 144.0000\n",
      "Epoch 35/300\n",
      "264/264 [==============================] - 2s 8ms/step - fake_discriminator: 1.2106 - real_discriminator: 1.2173 - penalty_discriminator: 0.0047 - d_loss: 0.0398 - fake_generator: -1.2109 - close_generator: 2.0740 - g_loss: -0.1739 - _timestamp: 1647441510.0000 - _runtime: 146.0000\n",
      "Epoch 36/300\n",
      "264/264 [==============================] - 3s 12ms/step - fake_discriminator: 1.5699 - real_discriminator: 1.5778 - penalty_discriminator: 0.0040 - d_loss: 0.0320 - fake_generator: -1.5870 - close_generator: 3.9929 - g_loss: 0.4095 - _timestamp: 1647441513.0000 - _runtime: 149.0000\n",
      "Epoch 37/300\n",
      "264/264 [==============================] - 3s 10ms/step - fake_discriminator: 2.1841 - real_discriminator: 2.2092 - penalty_discriminator: 0.0039 - d_loss: 0.0135 - fake_generator: -2.1571 - close_generator: 8.1215 - g_loss: 1.9037 - _timestamp: 1647441516.0000 - _runtime: 152.0000\n",
      "Epoch 38/300\n",
      "264/264 [==============================] - 3s 9ms/step - fake_discriminator: 1.7644 - real_discriminator: 1.7506 - penalty_discriminator: 0.0039 - d_loss: 0.0524 - fake_generator: -1.7497 - close_generator: 6.0489 - g_loss: 1.2747 - _timestamp: 1647441518.0000 - _runtime: 154.0000\n",
      "Epoch 39/300\n",
      "264/264 [==============================] - 3s 10ms/step - fake_discriminator: 1.6728 - real_discriminator: 1.6795 - penalty_discriminator: 0.0039 - d_loss: 0.0328 - fake_generator: -1.6989 - close_generator: 5.1070 - g_loss: 0.8545 - _timestamp: 1647441521.0000 - _runtime: 157.0000\n",
      "Epoch 40/300\n",
      "264/264 [==============================] - 3s 11ms/step - fake_discriminator: 1.8360 - real_discriminator: 1.8212 - penalty_discriminator: 0.0041 - d_loss: 0.0555 - fake_generator: -1.8484 - close_generator: 4.4227 - g_loss: 0.3629 - _timestamp: 1647441524.0000 - _runtime: 160.0000\n",
      "Epoch 41/300\n",
      "264/264 [==============================] - 2s 9ms/step - fake_discriminator: 1.3391 - real_discriminator: 1.3342 - penalty_discriminator: 0.0065 - d_loss: 0.0700 - fake_generator: -1.3273 - close_generator: 1.3218 - g_loss: -0.6664 - _timestamp: 1647441526.0000 - _runtime: 162.0000\n",
      "Epoch 42/300\n",
      "264/264 [==============================] - 3s 12ms/step - fake_discriminator: 1.3959 - real_discriminator: 1.3974 - penalty_discriminator: 0.0069 - d_loss: 0.0674 - fake_generator: -1.3946 - close_generator: 1.5588 - g_loss: -0.6153 - _timestamp: 1647441529.0000 - _runtime: 165.0000\n",
      "Epoch 43/300\n",
      "264/264 [==============================] - 3s 12ms/step - fake_discriminator: 1.6063 - real_discriminator: 1.6181 - penalty_discriminator: 0.0062 - d_loss: 0.0498 - fake_generator: -1.6316 - close_generator: 2.6644 - g_loss: -0.2994 - _timestamp: 1647441533.0000 - _runtime: 169.0000\n",
      "Epoch 44/300\n",
      "264/264 [==============================] - 3s 10ms/step - fake_discriminator: 1.6554 - real_discriminator: 1.6564 - penalty_discriminator: 0.0049 - d_loss: 0.0484 - fake_generator: -1.6525 - close_generator: 2.7844 - g_loss: -0.2604 - _timestamp: 1647441535.0000 - _runtime: 171.0000\n",
      "Epoch 45/300\n",
      "264/264 [==============================] - 3s 11ms/step - fake_discriminator: 1.7117 - real_discriminator: 1.7159 - penalty_discriminator: 0.0040 - d_loss: 0.0356 - fake_generator: -1.7148 - close_generator: 3.4421 - g_loss: 0.0063 - _timestamp: 1647441538.0000 - _runtime: 174.0000\n",
      "Epoch 46/300\n",
      "264/264 [==============================] - 3s 9ms/step - fake_discriminator: 1.5884 - real_discriminator: 1.6018 - penalty_discriminator: 0.0040 - d_loss: 0.0266 - fake_generator: -1.6046 - close_generator: 4.6894 - g_loss: 0.7401 - _timestamp: 1647441541.0000 - _runtime: 177.0000\n",
      "Epoch 47/300\n",
      "264/264 [==============================] - 2s 8ms/step - fake_discriminator: 1.4246 - real_discriminator: 1.4257 - penalty_discriminator: 0.0040 - d_loss: 0.0391 - fake_generator: -1.4277 - close_generator: 4.7526 - g_loss: 0.9486 - _timestamp: 1647441543.0000 - _runtime: 179.0000\n",
      "Epoch 48/300\n",
      "264/264 [==============================] - 4s 15ms/step - fake_discriminator: 1.4697 - real_discriminator: 1.4657 - penalty_discriminator: 0.0039 - d_loss: 0.0433 - fake_generator: -1.4881 - close_generator: 4.5118 - g_loss: 0.7678 - _timestamp: 1647441547.0000 - _runtime: 183.0000\n",
      "Epoch 49/300\n",
      "264/264 [==============================] - 3s 11ms/step - fake_discriminator: 1.3292 - real_discriminator: 1.3453 - penalty_discriminator: 0.0040 - d_loss: 0.0235 - fake_generator: -1.3411 - close_generator: 4.6047 - g_loss: 0.9613 - _timestamp: 1647441550.0000 - _runtime: 186.0000\n",
      "Epoch 50/300\n",
      "264/264 [==============================] - 3s 10ms/step - fake_discriminator: 1.7140 - real_discriminator: 1.7023 - penalty_discriminator: 0.0041 - d_loss: 0.0526 - fake_generator: -1.7118 - close_generator: 5.4080 - g_loss: 0.9922 - _timestamp: 1647441553.0000 - _runtime: 189.0000\n",
      "Epoch 51/300\n",
      "264/264 [==============================] - 2s 8ms/step - fake_discriminator: 1.6469 - real_discriminator: 1.6603 - penalty_discriminator: 0.0040 - d_loss: 0.0264 - fake_generator: -1.6505 - close_generator: 4.3303 - g_loss: 0.5147 - _timestamp: 1647441555.0000 - _runtime: 191.0000\n",
      "Epoch 52/300\n",
      "264/264 [==============================] - 3s 10ms/step - fake_discriminator: 1.6873 - real_discriminator: 1.6911 - penalty_discriminator: 0.0040 - d_loss: 0.0367 - fake_generator: -1.6451 - close_generator: 7.5576 - g_loss: 2.1337 - _timestamp: 1647441558.0000 - _runtime: 194.0000\n",
      "Epoch 53/300\n",
      "264/264 [==============================] - 3s 11ms/step - fake_discriminator: 1.5172 - real_discriminator: 1.4939 - penalty_discriminator: 0.0040 - d_loss: 0.0629 - fake_generator: -1.5172 - close_generator: 4.7438 - g_loss: 0.8547 - _timestamp: 1647441561.0000 - _runtime: 197.0000\n",
      "Epoch 54/300\n",
      "264/264 [==============================] - 2s 9ms/step - fake_discriminator: 1.3942 - real_discriminator: 1.3993 - penalty_discriminator: 0.0038 - d_loss: 0.0333 - fake_generator: -1.3813 - close_generator: 3.5911 - g_loss: 0.4143 - _timestamp: 1647441563.0000 - _runtime: 199.0000\n",
      "Epoch 55/300\n",
      "264/264 [==============================] - 2s 9ms/step - fake_discriminator: 1.4643 - real_discriminator: 1.4588 - penalty_discriminator: 0.0041 - d_loss: 0.0463 - fake_generator: -1.4777 - close_generator: 2.1615 - g_loss: -0.3969 - _timestamp: 1647441565.0000 - _runtime: 201.0000\n",
      "Epoch 56/300\n",
      "264/264 [==============================] - 2s 9ms/step - fake_discriminator: 1.1284 - real_discriminator: 1.1156 - penalty_discriminator: 0.0043 - d_loss: 0.0554 - fake_generator: -1.1223 - close_generator: 1.8192 - g_loss: -0.2127 - _timestamp: 1647441567.0000 - _runtime: 203.0000\n",
      "Epoch 57/300\n",
      "264/264 [==============================] - 3s 10ms/step - fake_discriminator: 0.9217 - real_discriminator: 0.9221 - penalty_discriminator: 0.0046 - d_loss: 0.0452 - fake_generator: -0.9109 - close_generator: 1.0291 - g_loss: -0.3964 - _timestamp: 1647441570.0000 - _runtime: 206.0000\n",
      "Epoch 58/300\n",
      "264/264 [==============================] - 3s 13ms/step - fake_discriminator: 0.8866 - real_discriminator: 0.8872 - penalty_discriminator: 0.0047 - d_loss: 0.0463 - fake_generator: -0.8787 - close_generator: 0.9790 - g_loss: -0.3892 - _timestamp: 1647441574.0000 - _runtime: 210.0000\n",
      "Epoch 59/300\n",
      "264/264 [==============================] - 2s 9ms/step - fake_discriminator: 1.0284 - real_discriminator: 1.0278 - penalty_discriminator: 0.0045 - d_loss: 0.0454 - fake_generator: -1.0356 - close_generator: 0.9260 - g_loss: -0.5727 - _timestamp: 1647441576.0000 - _runtime: 212.0000\n",
      "Epoch 60/300\n",
      "134/264 [==============>...............] - ETA: 1s - fake_discriminator: 1.1806 - real_discriminator: 1.1897 - penalty_discriminator: 0.0054 - d_loss: 0.0444 - fake_generator: -1.1822 - close_generator: 0.7613 - g_loss: -0.8016"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import sys\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM,\n",
    "    Bidirectional,\n",
    "    Concatenate,\n",
    "    Conv1D,\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Input,\n",
    "    LeakyReLU,\n",
    "    ReLU,\n",
    "    Reshape,\n",
    "    Layer,\n",
    ")\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "class WGANGP(Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # Following parameter and optimizer set as recommended in paper\n",
    "        self.n_critic = 5\n",
    "\n",
    "        # Build the generator and critic\n",
    "        self.generator = self.build_generator()\n",
    "        self.discriminator = self.build_discriminator()\n",
    "\n",
    "        generator_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=self.config[\"learning_rate_generator\"],\n",
    "            beta_1=self.config[\"beta1\"],\n",
    "            beta_2=self.config[\"beta2\"],\n",
    "        )\n",
    "        discriminator_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=self.config[\"learning_rate_discriminator\"],\n",
    "            beta_1=self.config[\"beta1\"],\n",
    "            beta_2=self.config[\"beta2\"],\n",
    "        )\n",
    "\n",
    "        self.compile(\n",
    "            d_optimizer=discriminator_optimizer,\n",
    "            g_optimizer=generator_optimizer,\n",
    "            g_loss_fn=self.generator_loss,\n",
    "            d_loss_fn=self.discriminator_loss,\n",
    "        )\n",
    "\n",
    "        # # -------------------------------\n",
    "        # # Construct Computational Graph\n",
    "        # #       for the Discriminator\n",
    "        # # -------------------------------\n",
    "\n",
    "        # # Freeze generator's layers while training discriminator\n",
    "        # self.generator.trainable = False\n",
    "\n",
    "        # # Image input (real sample)\n",
    "        # klines = Input(shape=(self.config[\"nb_previous_close\"], 29))\n",
    "\n",
    "        # # Generate image based of noise (fake sample)\n",
    "        # fake_close = self.generator(klines)\n",
    "        # real_close = Input(shape=(1, 1))\n",
    "        # multiple_closes = Input(shape=(self.config[\"nb_previous_close\"], 1))\n",
    "        # fake_closes = Concatenate(axis=1)([multiple_closes, fake_close])\n",
    "        # real_closes = Concatenate(axis=1)([multiple_closes, real_close])\n",
    "\n",
    "        # # Discriminator determines validity of the real and fake images\n",
    "        # fake = self.discriminator(fake_closes)\n",
    "        # valid = self.discriminator(real_closes)\n",
    "\n",
    "        # # Construct weighted average between real and fake images\n",
    "        # interpolated_closes = RandomWeightedAverage()([fake_closes, real_closes])\n",
    "        # # Determine validity of weighted sample\n",
    "        # validity_interpolated = self.discriminator(interpolated_closes)\n",
    "\n",
    "        # # Use Python partial to provide loss function with additional\n",
    "        # # 'averaged_samples' argument\n",
    "        # partial_gp_loss = partial(\n",
    "        #     self.gradient_penalty_loss, averaged_samples=interpolated_closes\n",
    "        # )\n",
    "        # partial_gp_loss.__name__ = \"gradient_penalty\"  # Keras requires function names\n",
    "\n",
    "        # self.critic_model = Model(\n",
    "        #     inputs=[real_img, z_disc], outputs=[valid, fake, validity_interpolated]\n",
    "        # )\n",
    "        # self.critic_model.compile(\n",
    "        #     loss=[self.wasserstein_loss, self.wasserstein_loss, partial_gp_loss],\n",
    "        #     optimizer=optimizer,\n",
    "        #     loss_weights=[1, 1, 10],\n",
    "        # )\n",
    "        # # -------------------------------\n",
    "        # # Construct Computational Graph\n",
    "        # #         for Generator\n",
    "        # # -------------------------------\n",
    "\n",
    "        # # For the generator we freeze the critic's layers\n",
    "        # self.critic.trainable = False\n",
    "        # self.generator.trainable = True\n",
    "\n",
    "        # # Sampled noise for input to generator\n",
    "        # z_gen = Input(shape=(self.latent_dim,))\n",
    "        # # Generate images based of noise\n",
    "        # img = self.generator(z_gen)\n",
    "        # # Discriminator determines validity\n",
    "        # valid = self.critic(img)\n",
    "        # # Defines generator model\n",
    "        # self.generator_model = Model(z_gen, valid)\n",
    "        # self.generator_model.compile(loss=self.wasserstein_loss, optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        inputs = Input(\n",
    "            shape=(self.config[\"nb_previous_close\"], 29),\n",
    "            batch_size=self.config[\"batch_size\"],\n",
    "        )\n",
    "        outputs = Conv1D(32, 2, activation=LeakyReLU(alpha=0.1))(inputs)\n",
    "        outputs = Bidirectional(LSTM(64, dropout=0.3, activation=ReLU()))(outputs)\n",
    "        outputs = Dense(64, activation=LeakyReLU(alpha=0.1))(outputs)\n",
    "        outputs = Dropout(0.2)(outputs)\n",
    "        outputs = Dense(32, activation=LeakyReLU(alpha=0.1))(outputs)\n",
    "        outputs = Dropout(0.2)(outputs)\n",
    "        outputs = Dense(1, activation=None)(outputs)\n",
    "        outputs = Reshape(target_shape=(1, 1))(outputs)\n",
    "        generator = Model(inputs=inputs, outputs=outputs, name=\"genrator\")\n",
    "        return generator\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        inputs = Input(\n",
    "            shape=(self.config[\"nb_previous_close\"] + 1, 1),\n",
    "            batch_size=self.config[\"batch_size\"],\n",
    "        )\n",
    "        outputs = Conv1D(32, 2, activation=LeakyReLU(alpha=0.1))(inputs)\n",
    "        outputs = Conv1D(64, 2, activation=LeakyReLU(alpha=0.1))(outputs)\n",
    "        outputs = Dense(64, activation=LeakyReLU(alpha=0.1))(outputs)\n",
    "        outputs = Dropout(0.2)(outputs)\n",
    "        outputs = Dense(32, activation=LeakyReLU(alpha=0.1))(outputs)\n",
    "        outputs = Dropout(0.2)(outputs)\n",
    "        outputs = Dense(1, activation=None)(outputs)\n",
    "        discriminator = Model(inputs=inputs, outputs=outputs, name=\"discriminator\")\n",
    "        return discriminator\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "        super().compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "\n",
    "    def discriminator_loss(self, real, fake, current_metrics={}):\n",
    "        real_loss = tf.reduce_mean(real)\n",
    "        fake_loss = tf.reduce_mean(fake)\n",
    "\n",
    "        current_metrics[\"real_discriminator\"] = real_loss\n",
    "        current_metrics[\"fake_discriminator\"] = fake_loss\n",
    "        return fake_loss - real_loss\n",
    "\n",
    "    # Define the loss functions for the generator.\n",
    "    def generator_loss(self, fake, real, current_metrics={}):\n",
    "        fake_loss = -tf.reduce_mean(fake)\n",
    "        far_loss = tf.keras.metrics.mean_squared_error(tf.squeeze(real), tf.squeeze(fake))\n",
    "        g_loss = fake_loss + 0.5 * far_loss\n",
    "        current_metrics[\"fake_generator\"] = fake_loss\n",
    "        current_metrics[\"close_generator\"] = far_loss\n",
    "        current_metrics[\"g_loss\"] = g_loss\n",
    "        return fake_loss\n",
    "\n",
    "    def gradient_penalty(self, real_images, fake_images, current_metrics={}):\n",
    "        \"\"\"Calculates the gradient penalty.\n",
    "\n",
    "        This loss is calculated on an interpolated image\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        alpha = tf.random.normal((self.config[\"batch_size\"], 1), 0.0, 1.0)\n",
    "        diff = fake_images - real_images\n",
    "\n",
    "        interpolated = real_images + tf.multiply(diff, alpha[:, tf.newaxis])\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            pred = self.discriminator(interpolated, training=True)\n",
    "\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "\n",
    "        current_metrics[\"penalty_discriminator\"] = gp\n",
    "        return gp\n",
    "\n",
    "    def update_metrics(self, metrics, current_metrics, reduction=None):\n",
    "        if reduction == \"mean\":\n",
    "            factor_reduction = self.config[\"n_discriminator\"]\n",
    "        else:\n",
    "            factor_reduction = 1\n",
    "\n",
    "        for name, value in current_metrics.items():\n",
    "            metrics[name] = metrics.get(name, 0) + value / factor_reduction\n",
    "\n",
    "    def train_step(self, data):\n",
    "        klines, previous_closes, real_close = data\n",
    "\n",
    "        metrics = {\n",
    "            \"fake_discriminator\": 0,\n",
    "            \"real_discriminator\": 0,\n",
    "            \"penalty_discriminator\": 0,\n",
    "            \"d_loss\": 0,\n",
    "            \"fake_generator\": 0,\n",
    "            \"close_generator\": 0,\n",
    "            \"g_loss\": 0\n",
    "        }\n",
    "        for _ in range(self.config[\"n_discriminator\"]):\n",
    "            current_metrics = {}\n",
    "            with tf.GradientTape() as tape:\n",
    "                fake_close = self.generator(klines, training=True)\n",
    "                fake_closes = tf.concat(\n",
    "                    [previous_closes, fake_close],\n",
    "                    axis=1,\n",
    "                )\n",
    "                real_closes = tf.concat([previous_closes, real_close], axis=1)\n",
    "                fake = self.discriminator(fake_closes, training=True)\n",
    "                real = self.discriminator(real_closes, training=True)\n",
    "                d_cost = self.d_loss_fn(real, fake, current_metrics)\n",
    "                gp = self.gradient_penalty(real_closes, fake_closes, current_metrics)\n",
    "                d_loss = d_cost + gp * self.config[\"gp_weight\"]\n",
    "                current_metrics[\"d_loss\"] = d_loss\n",
    "\n",
    "            self.update_metrics(metrics, current_metrics, reduction=\"mean\")\n",
    "\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(d_gradient, self.discriminator.trainable_variables)\n",
    "            )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            current_metrics = {}\n",
    "            fake_close = self.generator(klines, training=True)\n",
    "            fake_closes = tf.concat(\n",
    "                [previous_closes, fake_close],\n",
    "                axis=1,\n",
    "            )\n",
    "            fake = self.discriminator(fake_closes, training=True)\n",
    "            g_loss = self.g_loss_fn(fake, real, current_metrics)\n",
    "        self.update_metrics(metrics, current_metrics)\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        \n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "        return metrics\n",
    "\n",
    "config[\"gp_weight\"] = 10\n",
    "config[\"n_discriminator\"] = 5\n",
    "wgan = WGANGP(config)\n",
    "\n",
    "# Instantiate the optimizer for both networks\n",
    "# (learning_rate=0.0002, beta_1=0.5 are recommended\n",
    "\n",
    "# Define the loss functions for the discriminator,\n",
    "# which should be (fake_loss - real_loss).\n",
    "# We will add the gradient penalty later to this loss function.\n",
    "\n",
    "\n",
    "# Set the number of epochs for trainining.\n",
    "epochs = 300\n",
    "\n",
    "\n",
    "# Start training the model.\n",
    "wgan.fit(train_dataset, epochs=epochs, callbacks=[WandbCallback()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef2dec6b4e240eab5523f08814d3ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>close_generator</td><td>▁▂▁▁▂▃▃▅▇█</td></tr><tr><td>d_loss</td><td>▅▅▄▅█▇▁▂▁▃</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>fake_discriminator</td><td>▃▆▄▅▅▆██▂▁</td></tr><tr><td>fake_generator</td><td>▆▃▄▄▅▃▁▂▇█</td></tr><tr><td>g_loss</td><td>▅▃▄▃▅▃▁▂▇█</td></tr><tr><td>penalty_discriminator</td><td>▄▆▄▄█▆▁▂▂▃</td></tr><tr><td>real_discriminator</td><td>▃▆▅▅▄▆██▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>close_generator</td><td>0.1061</td></tr><tr><td>d_loss</td><td>0.11471</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>fake_discriminator</td><td>0.14777</td></tr><tr><td>fake_generator</td><td>-0.18572</td></tr><tr><td>g_loss</td><td>-0.13267</td></tr><tr><td>penalty_discriminator</td><td>0.01355</td></tr><tr><td>real_discriminator</td><td>0.1685</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">absurd-glade-3</strong>: <a href=\"https://wandb.ai/matiasetcheverry/crypto-prediction/runs/1bye0xdm\" target=\"_blank\">https://wandb.ai/matiasetcheverry/crypto-prediction/runs/1bye0xdm</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220316_153443-1bye0xdm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11996/1614899762.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         self.conv_layer = nn.Sequential(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv1d(config[\"nb_previous_close\"], 32, kernel_size=2),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.lstm_layer = nn.LSTM(\n",
    "            28, 64, num_layers=1, batch_first=True, bidirectional=True, dropout=0.3\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            # nn.Flatten(),\n",
    "            nn.Linear(in_features=2 * 64, out_features=64),\n",
    "            nn.LeakyReLU(),\n",
    "            # nn.Dropout2d(0.2),\n",
    "            nn.Linear(in_features=64, out_features=32),\n",
    "            nn.LeakyReLU(),\n",
    "            # nn.Dropout2d(0.2),\n",
    "            nn.Linear(in_features=32, out_features=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        output, (hidden_state, cell_state) = self.lstm_layer(x)\n",
    "        hidden_state = torch.permute(hidden_state, (1, 0, 2)).reshape(-1, 2 * 64)\n",
    "        x = self.fc_layers(hidden_state)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=2),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        _, nb_filters, width = self.conv_layer(torch.rand(1, 1, 21)).shape\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=nb_filters * width, out_features=64),\n",
    "            nn.LeakyReLU(),\n",
    "            # nn.Dropout2d(0.2),\n",
    "            nn.Linear(in_features=64, out_features=32),\n",
    "            nn.LeakyReLU(),\n",
    "            # nn.Dropout2d(0.2),\n",
    "            nn.Linear(in_features=32, out_features=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.conv_layer(z.unsqueeze(1))\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GAN(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # networks\n",
    "        self.generator = Generator(self.config)\n",
    "        self.discriminator = Discriminator(self.config)\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def adversarial_loss(self, y_hat, y):\n",
    "        return nn.BCELoss()(y_hat, y)\n",
    "\n",
    "    def generator_loss(self, fake, y_hat, y):\n",
    "        fake_generator = torch.mean(fake)\n",
    "        close_generator = torchmetrics.MeanSquaredError()(y_hat, y)\n",
    "        sign_generator = torch.mean(torch.abs(torch.sign(y_hat) - torch.sign(y)))\n",
    "        return fake_generator, close_generator, sign_generator\n",
    "\n",
    "    def discriminator_loss(self, real, fake, y_hat, y):\n",
    "        fake_discriminator = torch.mean(fake)\n",
    "        real_discriminator = torch.mean(real)\n",
    "        return fake_discriminator, real_discriminator\n",
    "\n",
    "    def _training_step_generator(self, batch, opt=None):\n",
    "        klines, multiple_close, y = batch\n",
    "        fake_close = self(klines)\n",
    "        fake = self.discriminator(torch.cat([multiple_close, fake_close], dim=1))\n",
    "        fake_generator, close_generator, sign_generator = self.generator_loss(\n",
    "            fake, fake_close, y\n",
    "        )\n",
    "        g_loss = -fake_generator + 0.5 * close_generator + 0.5 * sign_generator\n",
    "        if opt is not None:\n",
    "            opt.zero_grad()\n",
    "            self.manual_backward(g_loss)\n",
    "            opt.step()\n",
    "\n",
    "        return {\n",
    "            \"g_loss\": g_loss,\n",
    "            \"fake_generator\": fake_generator,\n",
    "            \"close_generator\": close_generator,\n",
    "            \"sign_generator\": sign_generator,\n",
    "        }\n",
    "\n",
    "    def _training_step_discriminator(self, batch, opt=None, steps=5):\n",
    "        klines, multiple_close, y = batch\n",
    "        D_loss = 0\n",
    "        Fake_discriminator = 0\n",
    "        Real_discriminator = 0\n",
    "        for _ in range(steps):\n",
    "            fake_close = self(klines)\n",
    "            fake = self.discriminator(torch.cat([multiple_close, fake_close], dim=1))\n",
    "            real = self.discriminator(torch.cat([multiple_close, y], dim=1))\n",
    "            fake_discriminator, real_discriminator = self.discriminator_loss(\n",
    "                real, fake, fake_close, y\n",
    "            )\n",
    "            d_loss = fake_discriminator - real_discriminator\n",
    "            if opt is not None:\n",
    "                opt.zero_grad()\n",
    "                self.manual_backward(d_loss)\n",
    "                opt.step()\n",
    "\n",
    "            D_loss += d_loss / steps\n",
    "            Fake_discriminator += fake_discriminator / steps\n",
    "            Real_discriminator += real_discriminator / steps\n",
    "\n",
    "        return {\n",
    "            \"d_loss\": D_loss,\n",
    "            \"fake_discriminator\": Fake_discriminator,\n",
    "            \"real_discriminator\": Real_discriminator,\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        opt_g, opt_d = self.optimizers()\n",
    "\n",
    "        metrics = {}\n",
    "        metrics.update(self._training_step_generator(batch, opt_g))\n",
    "        metrics.update(self._training_step_discriminator(batch, opt_d, steps=1))\n",
    "\n",
    "        self.log_dict(\n",
    "            metrics,\n",
    "            prog_bar=True,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        metrics = {}\n",
    "        metrics.update(self._training_step_generator(batch))\n",
    "        metrics.update(self._training_step_discriminator(batch, steps=1))\n",
    "        metrics = {\n",
    "            \"val_\" + metric_name: metric_value\n",
    "            for metric_name, metric_value in metrics.items()\n",
    "        }\n",
    "\n",
    "        self.log_dict(\n",
    "            metrics,\n",
    "            prog_bar=True,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "        return metrics\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt_g = torch.optim.Adam(\n",
    "            self.generator.parameters(),\n",
    "            lr=self.config[\"learning_rate_generator\"],\n",
    "            # betas=(self.config[\"beta1\"], self.config[\"beta2\"]),\n",
    "        )\n",
    "        opt_d = torch.optim.Adam(\n",
    "            self.discriminator.parameters(),\n",
    "            lr=self.config[\"learning_rate_discriminator\"],\n",
    "            betas=(self.config[\"beta1\"], self.config[\"beta2\"]),\n",
    "        )\n",
    "        return opt_g, opt_d\n",
    "\n",
    "\n",
    "model = GAN(config)\n",
    "\n",
    "model_checkpoint = pl.callbacks.model_checkpoint.ModelCheckpoint(\n",
    "    dirpath=run.dir if \"run\" in locals() else \"tmp/\",\n",
    "    filename=\"{epoch}-{val_loss:.3f}\",\n",
    "    monitor=\"_generatorg_loss\",\n",
    "    mode=\"min\",\n",
    "    verbose=True,\n",
    "    save_last=True,\n",
    ")\n",
    "\n",
    "script_checkpoint = training.ScriptCheckpoint(\n",
    "    dirpath=run.dir if \"run\" in locals() else \"tmp/\",\n",
    ")\n",
    "\n",
    "callbacks = [script_checkpoint]\n",
    "log = None\n",
    "if config[\"job_type\"] == \"train\" or False:\n",
    "    callbacks.append(model_checkpoint)\n",
    "    print(f\"[INFO]: saving models.\")\n",
    "else:\n",
    "    print(f\"[INFO]: not saving models.\")\n",
    "if config[\"job_type\"] == \"debug\":\n",
    "    log = \"all\"\n",
    "\n",
    "if config[\"log_wandb\"]:\n",
    "    wandb_logger = pl.loggers.WandbLogger()\n",
    "    wandb_logger.watch(model, log=log, log_graph=True)\n",
    "else:\n",
    "    wandb_logger = None\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=150,\n",
    "    callbacks=callbacks,\n",
    "    logger=wandb_logger,\n",
    "    devices=\"auto\",\n",
    "    accelerator=\"auto\",\n",
    "    #     limit_train_batches=3,\n",
    "    #     limit_val_batches=3,\n",
    ")\n",
    "trainer.fit(model, dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "compteur = 0\n",
    "for i in range(len(previous_x)):\n",
    "    for j in range(len(previous_x[i])):\n",
    "        if not torch.equal(previous_x[i][0], previous_x[i][j]):\n",
    "            compteur += 1\n",
    "print(compteur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "compteur = 0\n",
    "for i in range(len(preprevious_x)):\n",
    "    for j in range(len(preprevious_x[i])):\n",
    "        if not torch.equal(preprevious_x[i][0], preprevious_x[i][j]):\n",
    "            compteur += 1\n",
    "print(compteur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "compteur = 0\n",
    "for i in range(len(following_x)):\n",
    "    for j in range(len(following_x[i])):\n",
    "        if not torch.equal(following_x[i][0], following_x[i][j]):\n",
    "            compteur += 1\n",
    "print(compteur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14884/1864882645.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mklines1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mklines2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklines1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m print(\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "batch = previous_x[-1]\n",
    "klines1 = batch[14]\n",
    "klines2 = batch[15]\n",
    "print(klines1.shape)\n",
    "print(\n",
    "    model.generator.fc_layers(\n",
    "        klines1.unsqueeze(0),\n",
    "    )\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "321abdbcb709f963ff456ab1955c8b6ac962fdf45411881beed91e0e00a7d370"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit ('binance')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
