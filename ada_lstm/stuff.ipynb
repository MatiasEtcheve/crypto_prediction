{"cells":[{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["import sys\n","from pathlib import Path\n","\n","IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n","if IS_KAGGLE:\n","    repo_path = Path(\"../input/crypto_prediction\")\n","else:\n","    repo_path = Path(\"/home/matias/crypto_prediction\")\n","sys.path.append(str(repo_path))"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2022-03-18T20:22:15.220184Z","iopub.status.busy":"2022-03-18T20:22:15.219861Z","iopub.status.idle":"2022-03-18T20:22:15.233305Z","shell.execute_reply":"2022-03-18T20:22:15.232618Z","shell.execute_reply.started":"2022-03-18T20:22:15.220150Z"},"trusted":true},"outputs":[],"source":["from copy import deepcopy\n","from datetime import date, datetime, timedelta\n","from functools import partial\n","from importlib import reload\n","from pathlib import Path\n","\n","import numpy as np\n","import pandas as pd\n","import plotly.graph_objects as go\n","import plotly.io as pio\n","import tensorflow as tf\n","from datapoints import assets\n","from plotly.subplots import make_subplots\n","from query_datasets import get_data\n","from sklearn.base import BaseEstimator, ClassifierMixin\n","from sklearn.ensemble import AdaBoostRegressor\n","from sklearn.metrics import accuracy_score, precision_score, recall_score\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.utils import estimator_checks\n","from tensorflow.keras import layers\n","from tensorflow.keras.activations import sigmoid, tanh\n","from tensorflow.keras.metrics import Accuracy, Precision, Recall\n","from tensorflow.keras.models import Model\n","from tools import dataframe, training, wandb_api\n","from tqdm import tqdm\n","from wandb.keras import WandbCallback\n","\n","log_wandb = False\n","repo_path = Path().resolve().parent\n","# pio.renderers.default = \"browser\"\n"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2022-03-18T20:20:02.285076Z","iopub.status.busy":"2022-03-18T20:20:02.284804Z","iopub.status.idle":"2022-03-18T20:20:10.841531Z","shell.execute_reply":"2022-03-18T20:20:10.840786Z","shell.execute_reply.started":"2022-03-18T20:20:02.285044Z"},"trusted":true},"outputs":[],"source":["if log_wandb:\n","    import wandb\n","\n","    wandb_api.login()\n","    run = wandb.init(\n","        project=\"crypto_prediction\",\n","        group=\"Adaboost LSTM\",\n","        job_type=\"test\",\n","    )\n","    config = wandb.config\n","\n","else:\n","    config = {}\n","\n","config[\"job_type\"] = run.job_type if \"run\" in locals() else \"test\"\n","config[\"train_val_test_split\"] = [0.66, 1-0.66, 0]\n","config[\"interval\"] = \"1d\"\n","config[\"timesteps\"] = 8\n","config[\"lag\"] = 1\n","config[\"ago\"] = 3000\n","config[\"batch_size\"] = 64\n","config[\"learning_rate\"] = 0.0003"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["interesting_tickers = [\n","    # \"XRP\",\n","    # \"EOS\",\n","    # \"NEO\",\n","    # \"ALGO\",\n","    # \"SNX\",\n","    # \"ETH\",\n","    # \"AAVE\",\n","    # \"BNB\",\n","    \"BTC\",\n","    # \"DOT\",\n","    # \"XTZ\",\n","    # \"TRX\",\n","    # \"ADA\",\n","    # \"MATIC\",\n","    # \"DOGE\",\n","    # \"KLAY\",\n","    # \"AVAX\",\n","    # \"GRT\",\n","    # \"SAND\",\n","    # \"SOL\",\n","    # \"MANA\",\n","    # \"ATOM\",\n","    # \"VET\",\n","    # \"OMG\",\n","]\n"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["train_size: 1416, val_size: 729, test_size: 0\n","Length training dataset: 1\n","Length validation dataset: 1\n","Length test dataset: 1\n","Shape training sample: (1416, 152)\n","Shape training sample: 19.0\n","Shape validation sample: (729, 152)\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_10469/2414262426.py:6: PerformanceWarning:\n","\n","DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n","\n"]}],"source":["def compute_features(data, timesteps=8, lag=5):\n","    features = data.copy(deep=True)\n","\n","    for i in range(timesteps):\n","        for col in data.columns:\n","            features[f\"{col}_{i}\"] = features[col]  # .shift(i)\n","            # features[f\"open_{i}\"] = features.loc[:, \"Open\"].shift(i).pct_change()\n","            # features[f\"high_{i}\"] = features.loc[:, \"High\"].shift(i).pct_change()\n","            # features[f\"low_{i}\"] = features.loc[:, \"Low\"].shift(i).pct_change()\n","            # features[f\"close_{i}\"] = features.loc[:, \"Close\"].shift(i).pct_change()\n","            # features[f\"volume_{i}\"] = features.loc[:, \"Volume\"].shift(i).pct_change()\n","\n","    labels = features[\"Close\"].shift(-lag)  # - features[\"Open\"].shift(-1)\n","    features = features.drop(labels=data.columns, axis=1)\n","\n","    scaler = MinMaxScaler()\n","    features = features.replace(\n","        to_replace=[np.inf, -np.inf, float(\"inf\"), float(\"inf\")],\n","        value=0,\n","    ).dropna()\n","    df_scaled = scaler.fit_transform(features)\n","    df_scaled = pd.DataFrame(df_scaled, columns=features.columns, index=features.index)\n","    return df_scaled, labels\n","\n","\n","def create_asset(\n","    ticker,\n","    interval,\n","    beginning_date,\n","    ending_date,\n","    compute_features=lambda x: x,\n","):\n","\n","    klines = get_data.download_klines(\n","        ticker,\n","        interval,\n","        beginning_date=beginning_date,\n","        ending_date=ending_date,\n","        directory=repo_path / \"ada_lstm\" / \"tmp\",\n","    )\n","    trends = get_data.download_trends(\n","        ticker,\n","        interval,\n","        beginning_date=beginning_date,\n","        ending_date=ending_date,\n","        directory=repo_path / \"ada_lstm\" / \"tmp\",\n","    )\n","    blockchain_infos = get_data.download_blockchain(\n","        \"BTC\",\n","        interval,\n","        beginning_date=beginning_date,\n","        ending_date=ending_date,\n","        directory=repo_path / \"ada_lstm\" / \"tmp\",\n","    )\n","    santiment = get_data.download_santiment(\n","        \"BTC\",\n","        interval,\n","        beginning_date=beginning_date,\n","        ending_date=ending_date,\n","        directory=repo_path / \"ada_lstm\" / \"tmp\",\n","    )\n","    data = pd.concat([klines, blockchain_infos, trends, santiment], axis=1).astype(\n","        \"float32\"\n","    )\n","    data = data.replace(\n","        to_replace=[np.inf, -np.inf, float(\"inf\"), float(\"inf\")],\n","        value=0,\n","    )\n","\n","    features, labels = compute_features(data)\n","\n","    return assets.TrainAsset(\n","        ticker=ticker,\n","        df=data,\n","        labels=labels,\n","        features=features,\n","        interval=interval,\n","        compute_features=compute_features,\n","    )\n","\n","\n","class DataModule:\n","    def __init__(\n","        self,\n","        config,\n","        compute_features=None,\n","        inputs=None,\n","        save_klines=True,\n","    ):\n","        self.config = config\n","        self.compute_features = compute_features\n","        self.inputs = inputs\n","        self.save_klines = save_klines\n","\n","        self.setup()\n","\n","    def setup(self):\n","        self.train_datapoints = []\n","        for input in self.inputs:\n","            dp = create_asset(\n","                **input,\n","                interval=self.config[\"interval\"],\n","                compute_features=self.compute_features,\n","            )\n","            if dp == []:\n","                continue\n","            dp.df = dp.df.dropna()\n","            dp.labels = dp.labels.dropna()\n","            dp._features = dp._features.dropna()\n","\n","            common_index = dp.df.index.intersection(dp.labels.index)\n","            common_index = common_index.intersection(dp._features.index)\n","\n","            dp.df = dp.df.loc[common_index]\n","            dp.labels = dp.labels.loc[common_index]\n","            dp._features = dp._features.loc[common_index]\n","\n","            train_dp = assets.TrainAsset(\n","                ticker=input[\"ticker\"],\n","                df=dp.df,\n","                labels=dp.labels,\n","                features=dp._features,\n","                interval=self.config[\"interval\"],\n","                compute_features=self.compute_features,\n","            )\n","            if not train_dp.isempty:\n","                self.train_datapoints.append(train_dp)\n","            else:\n","                print(\n","                    f\"{dp.ticker} is empty from {input['beginning_date']} to {input['ending_date']}.\"\n","                )\n","\n","    def clean_datapoints(self, datapoints):\n","        return datapoints\n","\n","    def concat_and_shuffle(self, features, labels):\n","        assert len(features) == len(labels)\n","        _features = np.concatenate(features, axis=0)\n","        _labels = np.concatenate(labels, axis=0)\n","        assert len(_features) == len(_labels)\n","        p = np.random.permutation(len(_features))\n","        return _features[p], _labels[p]\n","\n","    def nest_train_test_val_split(\n","        self, datapoints, offset, train_size, val_size, test_size=0\n","    ):\n","        train_features = []\n","        train_labels = []\n","        val_features = []\n","        val_labels = []\n","        test_datapoints = {}\n","        for dp in datapoints:\n","            train_beginning = offset\n","            train_ending = train_beginning + train_size\n","            val_beginning = train_ending\n","            val_ending = val_beginning + val_size\n","\n","            test_beginning = val_beginning\n","            test_ending = val_ending\n","            # test_beginning = val_ending\n","            # test_ending = test_beginning + test_size\n","\n","            train_features.append(dp._features[train_beginning:train_ending])\n","            train_labels.append(dp.labels[train_beginning:train_ending])\n","            val_features.append(dp._features[val_beginning:val_ending])\n","            val_labels.append(dp.labels[val_beginning:val_ending])\n","\n","            test_datapoints[dp.ticker] = assets.TrainAsset(\n","                ticker=dp.ticker,\n","                df=dp.df.iloc[test_beginning:test_ending],\n","                labels=dp.labels.iloc[test_beginning:test_ending],\n","                features=dp._features.iloc[test_beginning:test_ending],\n","                interval=dp.interval,\n","                compute_features=dp.compute_features,\n","            )\n","\n","        return (\n","            self.concat_and_shuffle(train_features, train_labels),\n","            self.concat_and_shuffle(val_features, val_labels),\n","            test_datapoints,\n","        )\n","\n","    def _init_train_val_data(self, train_datapoints):\n","        train_datapoints = self.clean_datapoints(train_datapoints)\n","        if self.config[\"train_val_test_split\"][0] > 1:\n","            train_size = int(self.config[\"train_val_test_split\"][0])\n","        else:\n","            train_size = int(\n","                len(train_datapoints[0].df) * self.config[\"train_val_test_split\"][0]\n","            )\n","        if self.config[\"train_val_test_split\"][1] > 1:\n","            val_size = int(self.config[\"train_val_test_split\"][1])\n","        else:\n","            val_size = int(\n","                len(train_datapoints[0].df) * self.config[\"train_val_test_split\"][1]\n","            )\n","        if self.config[\"train_val_test_split\"][2] > 1:\n","            test_size = int(self.config[\"train_val_test_split\"][2])\n","        else:\n","            test_size = int(\n","                len(train_datapoints[0].df) * self.config[\"train_val_test_split\"][2]\n","            )\n","        print(f\"train_size: {train_size}, val_size: {val_size}, test_size: {test_size}\")\n","        max_offset = max(\n","            len(train_datapoints[0].df) - (train_size + val_size + test_size), 1\n","        )\n","        train_datasets = []\n","        val_datasets = []\n","        test_datapoints = []\n","        for offset in range(0, max_offset, val_size + test_size):\n","            train_dataset, val_dataset, test_datapoint = self.nest_train_test_val_split(\n","                train_datapoints, offset, train_size, val_size, test_size\n","            )\n","            train_datasets.append(train_dataset)\n","            val_datasets.append(val_dataset)\n","            test_datapoints.append(test_datapoint)\n","        return train_datasets, val_datasets, test_datapoints\n","\n","\n","config[\"job_type\"] = run.job_type if \"run\" in locals() else \"test\"\n","config[\"train_val_test_split\"] = [0.66, 1 - 0.66, 0]\n","config[\"interval\"] = \"1d\"\n","config[\"timesteps\"] = 8\n","config[\"lag\"] = 1\n","config[\"ago\"] = 3000\n","config[\"batch_size\"] = 64\n","config[\"learning_rate\"] = 0.0003\n","\n","inputs = [\n","    {\n","        \"ticker\": ticker,\n","        \"beginning_date\": datetime.combine(date.today(), datetime.min.time())\n","        - dataframe.convert_to_timedelta(config[\"interval\"], ago=config[\"ago\"]),\n","        \"ending_date\": datetime(\n","            2022, 5, 21\n","        ),  # datetime.combine(date.today(), datetime.min.time()),\n","    }\n","    for ticker in interesting_tickers\n","]\n","\n","dm = DataModule(\n","    config,\n","    partial(compute_features, timesteps=config[\"timesteps\"], lag=config[\"lag\"]),\n","    inputs,\n","    save_klines=True,\n",")\n","train_datasets, val_datasets, test_datapoints = dm._init_train_val_data(\n","    dm.train_datapoints\n",")\n","print(f\"Length training dataset: {len(train_datasets)}\")\n","print(f\"Length validation dataset: {len(train_datasets)}\")\n","print(f\"Length test dataset: {len(train_datasets)}\")\n","\n","print(f\"Shape training sample: {train_datasets[0][0].shape}\")\n","print(f\"Shape training sample: {train_datasets[0][0].shape[1] / config['timesteps']}\")\n","\n","print(f\"Shape validation sample: {val_datasets[0][0].shape}\")\n","config[\"input_size\"] = train_datasets[0][0].shape[1]\n","assert (\n","    config[\"input_size\"] // config[\"timesteps\"]\n","    == config[\"input_size\"] / config[\"timesteps\"]\n",")\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["class LSTMModel(Model):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","\n","        self.model = self.build_model()\n","\n","        self.compile(\n","            optimizer=tf.keras.optimizers.Adam(learning_rate=config[\"learning_rate\"]),\n","            loss=\"mse\",\n","            metrics=[\"mae\", \"mse\"],\n","        )\n","\n","    def build_model(self):\n","        inputs = layers.Input(\n","            shape=(self.config[\"input_size\"]),\n","        )\n","        outputs = layers.Reshape(\n","            (\n","                self.config[\"timesteps\"],\n","                -1,\n","            )\n","        )(inputs)\n","        outputs = layers.LSTM(512, activation=layers.ReLU(), return_sequences=True)(\n","            outputs\n","        )\n","        outputs = layers.LSTM(256, activation=layers.ReLU(), return_sequences=True)(\n","            outputs\n","        )\n","        outputs = layers.LSTM(128, activation=layers.ReLU(), dropout=0.3)(outputs)\n","        outputs = layers.Flatten()(outputs)\n","        outputs = layers.Dense(128, activation=layers.ReLU())(outputs)\n","        outputs = layers.Dense(1, activation=None)(outputs)\n","        return Model(inputs=inputs, outputs=outputs, name=\"model\")\n","\n","    def call(self, klines):\n","        return self.model(klines)\n","\n","\n","def metrics_precision(targets, predictions):\n","    bool_predictions = tf.math.greater(predictions, 0)\n","    bool_targets = tf.math.greater(targets, 0)\n","    tp = tf.math.reduce_sum(tf.cast(bool_predictions[bool_targets], tf.float32))\n","    return tp / tf.math.reduce_sum(tf.cast(bool_predictions, tf.float32))\n","\n","\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<__main__.KerasRegressor object at 0x7f4b6c13fb50>\n"]},{"data":{"text/plain":["AdaBoostRegressor(base_estimator=<__main__.KerasRegressor object at 0x7f4b6c30f640>,\n","                  learning_rate=0.2, loss='square', n_estimators=5,\n","                  random_state=42)"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["class KerasRegressor:\n","    def __init__(self, config, path, dm):\n","        self.config = config\n","        self.path = path\n","        self.dm = dm\n","\n","    def get_params(self, deep=True):\n","        return {\"config\": self.config, \"path\": self.path, \"dm\": self.dm}\n","\n","    def set_params(self, **kwargs):\n","        if \"config\" in kwargs.keys():\n","            self.config = kwargs[\"config\"]\n","        if \"path\" in kwargs.keys():\n","            self.path = kwargs[\"path\"]\n","        if \"dm\" in kwargs.keys():\n","            self.dm = kwargs[\"dm\"]\n","\n","    def fit(self, X, y, sample_weights=None):\n","        self.model = LSTMModel(self.config)\n","\n","\n","        if sample_weights is None:\n","            sample_weights = np.ones(len(X))\n","        train_data = (\n","            tf.data.Dataset.from_tensor_slices((X, y, sample_weights))\n","            .shuffle(len(X), reshuffle_each_iteration=True, seed=42)\n","            .batch(\n","                self.config[\"batch_size\"],\n","                drop_remainder=False,\n","                num_parallel_calls=tf.data.AUTOTUNE,\n","            )\n","        )\n","\n","        self.model.fit(\n","            train_data,\n","            epochs=25,\n","            verbose=False,\n","        )\n","        return self\n","\n","    def predict(self, X):\n","        return np.squeeze(self.model(X))\n","\n","\n","estimator = KerasRegressor(\n","    config, path=Path(run.dir) if \"run\" in locals() else Path(\".\"), dm=dm\n",")\n","ada_regressor = AdaBoostRegressor(\n","    base_estimator=KerasRegressor(\n","        config, path=Path(run.dir) if \"run\" in locals() else Path(\".\"), dm=dm\n","    ),\n","    n_estimators=5,\n","    random_state=42,\n","    loss=\"square\",\n","    learning_rate=0.2\n",")\n","ada_regressor.fit(train_datasets[0][0], train_datasets[0][1])\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["EOS: 1.0 \t 1.0 \t 1.0\n","AVERAGE\n","1.0 1.0 1.0\n"]}],"source":["for index, (classifier, test_datapoint) in enumerate(zip(classifiers, test_datapoints)):\n","    if index == 0:\n","        base_datapoints = {\n","            ticker: deepcopy(dp) for ticker, dp in test_datapoint.items()\n","        }\n","    else:\n","        for ticker, dp in test_datapoint.items():\n","            base_datapoints[ticker].df = pd.concat([base_datapoints[ticker].df, dp.df])\n","            base_datapoints[ticker].labels = pd.concat(\n","                [base_datapoints[ticker].labels, dp.labels]\n","            )\n","            base_datapoints[ticker]._features = pd.concat(\n","                (base_datapoints[ticker]._features, dp._features)\n","            )\n","\n","\n","for ticker, dp in base_datapoints.items():\n","    base_datapoints[ticker].predictions = ada_regressor.predict(dp.features)\n"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["m = 1\n","\n","fig = make_subplots(\n","    rows=m,\n","    cols=1,\n","    subplot_titles=[dp.ticker for dp in base_datapoints.values()],\n","    horizontal_spacing=0.0001,\n","    vertical_spacing=0.1,\n","    shared_xaxes=True,\n",")\n","\n","for index, (ticker, dp) in enumerate(base_datapoints.items()):\n","    if index >= m:\n","        break\n","    predictions = dp.predictions\n","    labels = dp.labels\n","    df = dp.df\n","\n","    fig.add_trace(\n","        go.Scatter(\n","            x=labels.index,\n","            y=labels,\n","            line=dict(color=\"black\", width=1),\n","            name=f\"{dp.ticker} close\",\n","        ),\n","        row=index + 1,\n","        col=1,\n","    )\n","\n","    for idx, estimator in enumerate(ada_regressor.estimators_):\n","        temp_predictions = estimator.predict(dp.features)\n","        fig.add_trace(\n","            go.Scatter(\n","                x=labels.index, \n","                y=np.squeeze(temp_predictions),\n","                name=f\"{idx} predictions\",\n","            ),\n","            row=index + 1,\n","            col=1,\n","        )\n","\n","    fig.add_trace(\n","        go.Scatter(\n","            x=labels.index, \n","            y=np.squeeze(predictions),\n","            # showlegend=False,\n","            line=dict(color=\"red\", width=1),\n","            name=f\"{dp.ticker} predictions\",\n","        ),\n","        row=index + 1,\n","        col=1,\n","    )\n","\n","fig.update_layout(height=450*m, width=1000, margin=dict(l=10, r=20, t=30, b=10))\n","fig.show()\n"]}],"metadata":{"interpreter":{"hash":"321abdbcb709f963ff456ab1955c8b6ac962fdf45411881beed91e0e00a7d370"},"kernelspec":{"display_name":"Python 3.9.8 64-bit ('binance')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.8"}},"nbformat":4,"nbformat_minor":4}
